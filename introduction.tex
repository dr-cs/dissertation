\chapter{Introduction}

This chapter sets the stage for the work presented in this dissertation.

\section{The Promise of AI and the Challenges of Software Engineering}


\subsection{Artificial Intelligence}

Artificial intelligence was one of the first grand promises of computing. Almost as soon as the field of computer science was born the pioneers of AI were promising machines that could think and act like humans within their lifetimes. Early research in AI focused on machines that could ``think'' like humans and employed symbolic computation to create constraint solvers, planners, and exert systems that used truth maintenance systems. We had computer programs that solved algebra equations, found paths, played games, and diagnosed illnesses based on user-reported symptoms. But symbolic, or knowlege-based AI hit a bottleneck in the 1980s -- commonly called the knowledge acquisition bottleneck -- and the promise faded, a period known as the ``AI Winter.''

Say something about symbolic problem solving is easy, getting a robot to balance on two legs is hard, visual object recognition is hard, etc.

Around the same time, though, engineers were continuing to develop the field of neural networks, which had largely been abandoned by the AI community after Marvin Minsky showed the limitations of early linear neural network models.

This stuff led to modern AI, which is centered around machine learning.

Most modern AI deals with pattern recognition.

The early vision of AI -- the creation of intelligent agents -- was lost in modern pattern recognition-oriented AI.

Reinforcement learning applies modern AI -- quantitative machine learning -- to the agent problem.

\subsection{Reinforcement Learning}

One can think of reinforcement learning (RL) as a machine learning approach to planning, that is, a way of finding a sequence of actions that achieves a goal.  The RL problem formulation is this: an agent's world is described by a set of states, the agent can execute one of a set of actions in each state, and the agent is rewarded to greater or lesser degrees for each state-changing action it executes. In RL, problems of decision-making by agents interacting with uncertain environments are usually modeled as Markov decision problems (MDPs). In the MDP framework, at each time step the agent senses the state of the environment and executes an action from the set of actions available to it in that state. The agent's action (and perhaps other uncontrolled external events) cause a stochastic change in the state of the environment. The agent receives a (possibly zero) scalar reward each time it executes an action and makes a transition to a new state. The agent's goal is to find a {\it policy} that says which action should be chosen in each state.  The policy should specify actions that maximize the expected sum of rewards over some time horizon. An optimal policy is a mapping from states to actions that maximizes the long-term expected reward.  In short, a policy defines which action an agent should take in a given state to maximize its chances of reaching a goal.  Reinforcement learning is a large and active area of research, but the preceding is all the reader needs to understand the work presented here. For the software engineer who would like to employ refinforcement learning without becoming an expert in reinforcement learning the most important thing to understand about reinforcement learning is that the world of an agent can be modeled in terms of states, actions, and rewards.

More detail can be found in \cite{sutton1998reinforcement,kaelbling1996reinforcement}.

\subsection{Software Engineering}

While the field of AI went though its periods of promise, disappointment, and redefinition the rest of computer science went about the business of creating hardware and software systems for a variety of applications, initially business management and scientific computing.  Since the personal computing revolution, a great deal of effort has been put into making software systems easy for all kinds of users, technical and non-technical.  Through it all, computing power has increased dramatically and the software systems users demand have exploded in size and complexity.  Software engineering has struggled to keep pace with the growing size and complexity of these systems. Today the field of software engineering, both in academia and industry, has developed a well-defined set of practices and design guidelines that result in software systems that are maintainable, reliable and extensible.


\subsection{Programming Langauges}

Programming languages have been the primary means by which research in software engineering and formal computer science has been brought to bear for the working programmer. From structured programming to object-oriented programming to powerful modern type systems, important advances in computing research have real impact when they are incorporated as features in practical programming languages. In the same way that, say, formal methods are used by the modern programmer in the form of static type systems without requiring the programmer to know much about formal methods, AFABL's goal is to allow the programmer to use reinforcement learning without knowing much about reinforcement learning algorithms.

\section{Contributions}

The primary contribution of this work is to marry AI and software engineering in a way that advances both fields. The needs of practical software engineering inspires a new AI algorithm for modular reinforcement learning. Integrating this new formulation of MRL and associated algorithms in to a programming language enables a new kind of software engineering: modular agent programming.


\section{Overview}
