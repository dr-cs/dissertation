\chapter{Introduction}

\begin{quote}
\added[id=v2]{If you don't know where you are going, any road will take you there.\\
  -- Lewis Carroll, paraphrased from Alice's Adventures in Wonderland}
\end{quote}


This chapter sets the stage for the work presented in this dissertation, an overview of its contributions, and concludes with a road-map of the following chapters.


\section{The Dream of AI}

Artificial intelligence was one of the first grand promises of computing. Almost as soon as the field of computer science was born the pioneers of AI were promising machines that could think and act as well as humans within the ``visible future'' (Herbert Simon, quoted in 1958). Early research in AI focused on machines that could ``think'' like humans and employed symbolic computation to create constraint solvers, planners, and exert systems that used truth maintenance systems to maintain sets of facts and inference rules that produced new knowledge. We had computer programs that solved algebra equations, found paths, played games, and diagnosed illnesses based on user-reported symptoms. Symbolic, or knowledge-based AI hit a bottleneck in the late 1980s -- commonly called the knowledge acquisition bottleneck -- and the early promise of developing systems that {\it replaced} humans faded. But AI in general did not fade. AI reinvented itself. Instead of creating systems of rules and inference engines based on encoded knowledge, modern AI applies well-developed models from mathematics and engineering -- vector space models for text retrieval, hidden Markov models for speech recognition, neural networks (though originally invented within AI but then abandoned and developed by electrical engineers) for image recognition -- to problems traditionally considered part of AI. In modern AI the emphasis is on rationality -- achieving optimal results without trying to explicitly mimic human cognition -- and in developing systems that assist humans in some way. In modern AI every approach must have a performance measure that is being optimized -- speech phoneme recognition rate, generalization error in image classifiers, etc. -- and AI algorithms are not accepted if they do not either define a new category rigorously, or rigorously compare their performance to existing algorithms using accepted performance criteria. Most AI algorithms today are concerned with focused problems and find application in software that assists humans, such as helping humans find the right books to buy using collaborative filtering or finding photos of their babies using facial image recognition.  Reinforcement learning is notable for applying modern AI approaches to the age old intelligent agent problem: given the state of the world and all the actions an agent {\it could} take, which action {\it should} the agent take.

One can think of reinforcement learning (RL) as a machine learning approach to planning, that is, a way of finding a sequence of actions that achieves a goal.  The RL problem formulation is this: an agent's world is described by a set of states, the agent can execute an action from a prescribed set of actions in each state, and the agent is rewarded to greater or lesser degrees for each state-changing action it executes. The performance measure being optimized by reinforcement learning algorithms is long-term expected reward. So a reinforcement learning agent learns delayed gratification. For example, eating cake now provides high immediate reward but low long-term reward. A reinforcement learning agent learns to choose salad over cake (unless the agent is Ron Swanson). For the software engineer who would like to employ reinforcement learning without becoming an expert the most important thing to understand is that the world of an agent can be modeled in terms of states, actions, and one-step rewards. Our work shows that if you can specify the states, actions, and rewards for an agent our algorithms can work behind the scenes to develop a control policy.

\section{The Challenges of Software Engineering}

Software engineering has struggled to keep pace with the growing size and complexity of the systems being demanded by users. Over time the field of software engineering, both in academia and industry, has developed a well-defined set of practices and design guidelines that result in software systems that are maintainable, reliable and extensible. Programming languages have been a primary means by which research in software engineering and formal computer science has been brought to bear for the working programmer. From structured programming to object-oriented programming to powerful modern type systems, important advances in computing research have real impact when they are incorporated as features in practical programming languages. In the same way that, say, formal methods are used by the modern programmer in the form of static type systems without requiring the programmer to know much about formal methods, AFABL's goal is to allow the programmer to use reinforcement learning without knowing much about reinforcement learning algorithms.

\section{The Promise of Adaptive Partial Programming}

Our work in integrating reinforcement learning into a programming language is based on the idea of partial programming developed by researchers in hierarchical reinforcement learning. Partial programming is a framework for programming in which a programmer or designer specifies the structure of certain parts of the system while leaving other portions unspecified, such that a learning system can learn how to perform them. Hierarchical reinforcement learning defines closed-loop policies that group action sequences into logical units -- subroutines -- that achieve intermediate goals. For example, if the ultimate goal of an agent is to leave a building by exiting a room and walking down an hall, and the agent's primitive actions are MOVE-FORWARD, MOVE-LEFT, TURN-RIGHT and so-on, then one of the agent's subroutines might be FIND-DOOR, which achieves the intermediate goal of exiting the room. Partial programming system allows the agent designer to specify intermediate goals, write code to achieve certain goals, and let reinforcement learning algorithms learn how to achieve the others.

Hierarchical reinforcement learning decomposes the reinforcement learning problem temporally, that is, it breaks action sequences into subsequences. Another kind of adaptive programming -- known as modular reinforcement learning (MRL) -- is based on concurrent problem decomposition in which an agent may take only one action at a time but must pursue several goals simultaneously. MRL is especially well suited to continuing problems in which an agent is never ``done'' pursuing certain goals. For example, for the entire time it is operating a self-driving car will need to avoid other vehicles, optimize speeds for road conditions subject to speed limits, maximize fuel economy, minimize travel time, and so on.

MRL is a developing field with few algorithms and a small number of programming systems attempting to make MRL usable for working programmers. Currently available algorithms for MRL and programming systems based on them suffer from a reward coupling problem: modules used within the same agent must be authored together to ensure reward scales are comparable. This reward coupling is an impediment to module reuse in practical software engineering because an existing module can not simply be reused in a new context without re-engineering its reward function. Our work solves this problem with a reformulation of MRL and an algorithm that makes module reuse possible, and integrates this new kind of MRL in a practical programming language.


\section{Contributions}

The work presented in this dissertation marries artificial intelligence and software engineering in a way that advances both fields. The needs of practical software engineering for reuse and composability inspires a new AI algorithm for modular reinforcement learning. Integrating this new formulation of modular reinforcement learning and associated algorithms into a programming language enables a new kind of software engineering: modular adaptive agent programming. In particular, this work makes the following contributions:

\begin{itemize}
\item We explain a problem with the current state of the art in modular reinforcement learning, namely, that performance degrades if modules have differing, incomparable reward scales.
\item We empirically demonstrate the performance degradation of modular reinforcement learning agents whose modules have incomparable reward scales.
\item We present an analysis of the shortcoming of current approaches to modular reinforcement learning based on Arrow's Impossibility Theorem for social choice in order to frame our solution.
\item We reformulate the modular reinforcement learning problem as one of {\it command arbitration} instead of merging MDPs or Q-functions.
\item We present a command arbitration algorithm -- Arbi-Q -- that uses our theoretically grounded reformulation of modular reinforcement learning.
\item We empirically demonstrate that modular reinforcement learning agents using Arbi-Q exhibit no performance degradation when modules have incomparable reward scales.
\item We present a Scala-embedded domain-specific language -- AFABL -- that integrates modular reinforcement learning and our Arbi-Q command arbitration algorithm.
\item We demonstrate and quantify the value of integrating modular reinforcement learning into a programming language to practical software engineering in a programmer study applying AFABL in a synthetic agent programming domain.
\item We apply AFABL to a practical problem in psychology-based human agent modeling to demonstrate AFABL's practical potential.
\end{itemize}

\section{Outline}

In the following chapters we present the two major contributions of this dissertation: command arbitration for modular reinforcement learning and a practical agent programming language that integrates modular reinforcement learning: AFABL. For both major contributions there is a chapter providing the necessary background, then a chapter presenting our contributions. Related work is provided in each chapter where it is most relevant rather than gathered in a single place.

Chapter \ref{ch:rl} provides background information in modular reinforcement learning and existing approaches to modular reinforcement learning.

Chapter \ref{ch:arbiq} presents an empirical demonstration of the performance degradation of modular reinforcement learning agents whose modules have incomparable reward scales. Arrow's Impossibility Theorem for social choice provides an explanation for the failure of existing approaches to modular reinforcement learning and a framework for our solution. We present our solution, the Arbi-Q command arbitration algorithm, and empirically demonstrate that it does not exhibit the same performance degradation as existing approaches to modular reinforcement learning.

Chapter \ref{ch:se} provides background information on software engineering that motivates the use of modular reinforcement learning in building practical software systems, and the integration of modular reinforcement learning into a programming language.

Chapter \ref{ch:afabl} presents a programming language, AFABL, which integrates modular reinforcement learning. AFABL, a domain-specific language embedded in the Scala language, allows programmers to write adaptive software agents in a declarative style using elements of modular reinforcement learning: modules with states, actions, and rewards. We present the results of a programmer study that shows the value of integrating reinforcement learning into a programming language: AFABL agents are less complex, easier to write, and easier to adapt to changes in the environment.

Chapter \ref{ch:applications} presents a practical application of AFABL that further demonstrates the usefulness of integrating modular reinforcement learning into a programming language.

Chapter \ref{ch:conclusion} concludes the dissertation by reviewing how the central theses of this dissertation were confirmed and the present work's context, limitations, and consequences and discuss directions for future work.
