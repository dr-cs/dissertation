\chapter{Conclusion}\label{ch:conclusion}

\section{Review of Major Contributions}

This dissertation has reported on two primary contributions: a command arbitration algorithm for robust modular reinforcement learning, and a domain-specific language that integrates modular reinforcement learning, AFABL. The benefits of langauge-integrated reinforcement learning have been demonstrated in a study of programmers using AFABL compared to using a traditional programming language for the same tasks. AFABL agents are easier to write, are expressed in less complex code, and have more readily reused components than agents written in traditional programming languages. In the remainder of this chapter we discuss some implications of AFABL (as a representative first step in language-integrated modular reinforcement learning), limitations of the current work, and directions for future work.

\section{Adaptive Agent Software Engineering with AFABL}

In this section we discuss the broader implications of adaptive agent programming with AFABL. For the present discussion we define authorability is the ease with which a programmer can author the behavior of agents.  Concretely, we suggest a framework for assessing authorability that considers domain knowledge requirements, algorithm knowledge requirements, and the adaptability of agents.

{\bf Domain knowledge} refers to the world-specific details the agent author must program into the agent for a particular domain.  Examples of domain knowledge include representations of state and the dynamics of the world, that is, how actions cause transitions from one state to another.

{\bf Algorithm knowledge} refers to the degree of algorithm detail that must be programmed in the agent.  An agent using a general-purpose programming language with no libraries to support agent programming would need to write the agent's behavior algorithms from scratch. Even an agent using an agent programming library would still likely need to encode a significant amount of algorithm knowledge in the agent. For example, an agent agent that uses a STRIPS planner for action selection would need to contain details of STRIPS operators and the mechanisms for selecting them in response to state perception.

{\bf Adaptability} refers to the ease with which an agent, once authored, can adapt to a changing world or be reused in a different world.

These factors are not completely orthogonal.  High domain knowledge requirements can hinder adaptability because agent agents need to be preprogrammed for worlds that have different dynamics.  Domain knowledge and algorithm knowledge often go hand-in-hand, for example in the encoding of heuristic functions.  Returning to our STRIPS example, STRIPS operators essentially encode world dynamics into the decision making algorithm, thereby coupling domain knowledge and algorithm knowledge.

We say that authorability is high when required domain knowledge is low, algorithm knowledge is low, and adaptability is high.  Such an agent is easy to program in the first place and can be reused in new worlds with minimal reprogramming.

With this working framework for assessing the authorability of agent programming approaches we can map the agent programming space as a spectrum from fully scripted to fully learning approaches.

\subsubsection{Fully scripted agent Programming}

Fully scripted agents are the most common type of agents.  The scripts that control such agents specify every detail of the agent's behavior ahead of time.  While scripted agents can pursue goals and exhibit intelligence, the manner in which these goals are pursued must be written explicitly by the agent author, and if these goals are to be pursued using a particular algorithm, such as a planning algorithm, the algorithm itself must be encoded (or used as a library) from within the code.

In terms of our authorability framework, fully scripted agents have the following properties.

\begin{itemize}
\item Domain knowledge: high. A fully scripted agent must specify a knowledge representation for perception and action that facilitates all the kinds of analyses and decisions the agent will make.  The dynamics of the world must be known in advance and encoded in he script to allow the agent to pursue goals.
\item Algorithm knowledge: high.  Although the algorithms may be simple, such as big if-else ladders, the agent author must have complete knowledge of how the agent's behavior algorithms work.  More complex algorithms mean more complex knowledge for the agent author to manage, fully scripted approaches scale poorly to more complex agents.
\item Adaptability: low.  Once an agent is fully scripted for a given environment, it must be reprogrammed for new environments with different dynamics.  Also, any run-time adaptivity must be scripted explicitly.
\end{itemize}

\subsubsection{Fully Machine Learning agent Programming}

\begin{itemize}
\item Domain knowledge: low. With a sufficiently abstract state representation, the agent can have very little domain knowledge.
\item Algorithm knowledge: low to moderate.  The choice of machine learning algorithm and state representation determine the level of algorithm knowledge necessary to author a fully machine learning agent.
\item Adaptability: high.  Adaptability is the key advantage of machine learning.
\end{itemize}

Neither of these two endpoints of the agent authorability spectrum is desirable.  Fully scripted agents are laborious to write.  Fully machine learning agents typically exhibit a long period of decreasing incompetence until their learning algorithms have sufficient data.  What we want is something in between fully scripted and fully machine learning agents.


{\bf AFABL Hits the Agent Authorability Sweet Spot}

AFABL's integrated reinforcement learning separates the dynamics of the world from the action-selection logic in the agent, freeing the programmer from writing domain-dependent code and facilitating the adaptation of agents to new worlds.

\begin{itemize}
\item Domain knowledge: as much or as little as you want.  You can program the parts you know how to program, and leave AFABL to learn the rest automatically.
\item Algorithm knowledge: moderate.  AFABL is based on the agent and reinforcement learning models.  Behaviors are programmed as actions that execute in response to observed state (hence ``behavior''), and automatic behaviors additionally specify reward signals that enable AFABL to learn the best responses to particular states.
\item Adaptability: high.
\end{itemize}


\section{Limitations of Current Work}

% The work presented here is promising, but focused in scope.

\subsection{Reward Authoring}

As many other researchers have noted, reward authoring is not straightforward for programmers not trained in reinforcement learning. Study participants spent much of their AFABL writing time trying out different reward structures in an effort to improve their agents' performance. Although we provided documentation with hints on how to author reward structures, writing good reward functions is too opaque for most programmers. In the next section we discuss a possible improvement to AFABL which would relieve programmers from writing the reward functions of modules.

\subsection{Training}

Using any reinforcement learning-based programming system requires the availability of a simulation environment to train the learning modules before being used ``in production.'' Using an untrained reinforcement learning agent and accepting that it will perform poorly until it learns is not practical because reinforcement learning algorithms typically require hundreds or thousands of iterations to reach an acceptable level of performance. Separate modules with local state abstractions and reward functions help speed up training, but finding good factorizations into modules is a potentially steep burden to place on the programmer for larger agents.

\subsection{Host Language Limitations}

Writing AFABL agents is writing Scala code, so AFABL programmers must have at least basic competence with Scala and the Scala tool chain. Since it was outside the scope of the present work, we did not try to determine how much of the Scala tooling can be hidden or automated for AFABL programmers. We required study participants to use a recent version of IntelliJ IDEA and provided a pre-configured Scala/AFABL project and an IntelliJ plugin to automate the time tracking and submission process. Still, several participants had trouble running the study code smoothely, as is often the case with development tools. Many study participants who did not participate in a group session simply abandonded the study. We advertised the study to the Atlanta Scala Meetup, a group of local software engineers either using Scala professionally or interested in learning. Approximately 15 Scala Meetup members started the study and only one finished. Due to the amount of individual software issues I needed to help participants solve -- differing operationg systems, IntelliJ versions, etc -- I believe many of these dropouts were due to simple software setup issues.

In addition to Scala tooling issues, AFABL programmers must deal with the Scala programming language. For example, when programmer makes a mistake in their code the error messages come from the Scala compiler and run-time system, not AFABL. Luckily, in the study few people had such issues with AFABL code itself. With more complex agents problems are more likely to occur, and the programmer may be faced with the famous complexity of the Scala type system. The AFABL programmer who is not also a competent Scala programmer has little hope of debugging non-trivial errors.

\section{Directions for Future Work}


\subsection{Refined Module Types}

AFABL currently supports a narrow definition of an agent: a behaving entity with a set of states that must constantly be pursued or avoided. In reinforcement learning these kinds of modules are called called continuing tasks, as opposed to episodic goals. Previous version of AFABL supported a greater set of features but we removed them to focus on AFABL's core for the purpose of this work. With a cleaner core AFABL we could re-implement the features discussed below.

\paragraph{Drives}

A Drive is a behavior module that runs throughout the life of its conataining agent and represents a state that an agent should constantly seek.

\paragraph{Aversions}

An Aversion module is a behavior module that runs throughout the life of its conataining agent and represents a state that an agent should constantly avoid.  It is a constraint in the sense that, in certain states, a constraint module will identify actions that should *not* be executed.

\paragraph{Objectives}

A an objective is a short-term goal state that generates a drive module that is active until its goal is achieved.  The command arbitrator gives objective modules priority over drive modules, but all modules are constrained by constraint modules.

\paragraph{Tasks}

A task is a temporally-extended action, a "mini-policy" that acheives a subgoal.  Tasks are equivalent to subtasks (MaxQ), abstract machines (PHAM), or options from hierarchical reinforcement learning. Tasks could be manually authored, or algorithms from hierarchical reinforcement learning could be integrated into AFABL.

\subsection{Simplified Syntax}

The features listed above may make it possible to automatically author reward functions for modules. For example, the Bunny agent for Task 2 from Chapter \ref{ch:afabl} could be written with Drives and Aversions as follows:

\begin{lstlisting}[language=Scala]
  val afablBunny2 = AfablAgent(

    world = bunnyWorld,

    drives = Drives(state: BunnyState) {
      (BunnyState.bunny == BunnyState.food),
      (BunnyState.bunny == BunnyState.mate)
    },

    aversions = Aversions(state: BunnyState) {
      (BunnyState.bunny == BunnyState.wolf),
    }

    agentLevelReward = (state: BunnyState) => {
      if (state.bunny == state.wolf) 0.0
      else if (state.bunny == state.food) 1.0
      else if (state.bunny == state.mate) 1.0
      else 0.5
    }
  )
\end{lstlisting}

Instead of writing code to specify modules, the programmer specifies states that are to be constantly sought or avoided -- expressed as state predicates -- and the modules are derived from them automatically. Note also that this proposed syntax does not include state abstraction functions in modules becuase they could be derived automatically from the states that are to be sought or avoided.

\subsubsection{Drama Manager Support}

The features discussed above would go along way toward supporting drama managers for intelligent interactive narratives. In addition, a drama manager would need to be able to activate and deactivate modules and inject new objectives to support particular story goals.

\subsection{General Agent Architecture}

The current version of AFABL focuses on integrated reinforcement learning but could easily be extended to support integrated intelligence, that is, mixing of agent modules that employ differnt kinds of AI algorithms. Because an AFABL agent performs command arbitration over modules that support a behavioral interface (providing an action given a state observation) as opposed to merging elements of reinformcenet learners (like Q-values), the modules themselves can employ any mechanism to decide on actions given a state.  This information hiding means that AFABL agents could be composed of a mixture of modules that use many different kinds of AI, including statistical learning, rule-based reasoning, or (reactive) planning.  In this sense AFABL would be an integrated intelligence architecture.

\paragraph{Knowledge-Based Arbitrators}

In addition to the modules the arbitrator itself could employ different kinds of algorithms for command arbitration. A knowledge-based arbitrator could use hand-coded logic to decide from among the actions recommended by an agent's modules.  Simple arbitrators with few modules to arbitrate can often be coded quite simply as knowledge-based arbitrators.

\paragraph{Hierarchical Decomposition}

Beacuse modules are themselves agents, modules can contain other modules and perform command arbitration over those modules just as the top-level agent does.  Agents can thus be decomposed recursively into behavioral subsystems.  This recursive behavior module decomposition would provide the agent designer with great flexibility.  Recursive module composition is somewhat similar to the levels of competence in Brooks's subsumption architecture with an important difference: the internal workings of modules are never altered externally.  Modules are treated as black-boxes.  Command arbitration accomplishes the same result that output suppression does in classic subsumption.

\subsection{Independent (Non-Embedded) Language}\label{sec:conclusion-full-language}

Finally, once the additions to the language are integrated into the internal DSL and studied and refined sufficiently, an external DSL could be considered. Although an external DSL is far more work to implement, the benefits could justify the cost. A stand-alone version of AFABL would have its own set of development tools, report agent-oriented error messages to the user, and potentially run faster than equivalent internal DSL code.
