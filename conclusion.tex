\chapter{Conclusion}\label{ch:conclusion}


\section{Review of Major Contributions}

\subsection{Arbi-Q}

PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER

\subsection{AFABL}

PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER PLACEHOLDER

\section{Limitations of Current Work}

\subsection{Authoring and Training}

Using any reinforcement learning-based programming system requires the availability of a simulation environment to train the learning modules before being used ``in production.'' Using an untrained reinforcement learning agent and accepting that it will perform poorly until it learns is not practical because reinforcement learning algorithms typically require hundreds or thousands of iterations to reach an acceptable level of performance.

\subsection{Host Language Limitations}

Writing AFABL agents is writing Scala code.

Must have at least basic competence with Scala tool chain.

The error messages are under the control of the Scala compiler and run-time system.

\section{Directions for Future Work}

\subsection{General Agent Architecture}


\paragraph{Knowledge-Based Arbitrators}

A knowledge-based arbitrator uses hand-coded logic to decide from
among the actions recommended by a set of modules.  Simple arbetrators
with few modules to arbitrate can often be coded quite simply as
knowledge-based arbitrators.  For example, consider an arbitrator for
a simple agent consisting of two modules: AvoidWolf and FindFood.  A
command arbitrator could contain simple logic to encode the rule "when
the wolf is near, choose AvoidWolf's action, otherwise choose
FindFood's action."


\paragraph{Adaptive Arbitrators}

Adaptive arbitrators use learning algorithms under the hood to
automatically select actions from among a set of candidate actions.
Like adaptive behavior modules, adpative arbitrators must be trained
in the world with the set of modules it must arbitrate.  In effect, an
adaptvei arbitrator learns which module to listen to in a given state.


\subsubsection{Integrated Intelligence}

Because an AFABL agent performs command arbitration over modules that
support a behavioral interface (providing an action given a state
observation), the modules themselves can employ any mechanism to
decide on actions given a state.  This information hiding means that
AFABL agents can be composed of a mixture of modules that use many
different kinds of AI, including statistical learning, rule-based
reasoning, or (reactive) planning.  AFABL is an integrated
intelligence architecture.



\subsubsection{Hierarchical Decomposition}


Beacuse modules are themselves agents, modules can contain other
modules and perform command arbitration over those modules just as the
top-level agent does.  Agents can thus be decomposed recursively into
behavioral subsystems.  This recursive behavior module decomposition
provides the agent designer with great flexibility.  Recursive module
composition is somewhat similar to the levels of competence in
Brooks's subsumption architecture with an important difference: the
internal workings of modules are never altered externally.  Modules
are treated as black-boxes.  Command arbitration accomplishes the same
result that output suppression does in classic subsumption.

\begin{itemize}

\item Task

  A task is a temporally-extended action, a "mini-policy" that acheives a
  subgoal.  Tasks are equivalent to subtasks (MaxQ), abstract machines
  (PHAM), or options from hierarchical reinforcement learning.

\item Constraint Module

  A constraint module is a behavior module that runs throughout the
  life of its containing agent and represents a state that an agent
  should constantly avoid.  It is a constraint in the sense that, in
  certain states, a constraint module will identify actions that should
  *not* be executed.

\item Drive Module

  A drive module is a behavior module that runs throughout the
  life of its containing agent and represents states that an agent
  should constantly seek.

\item Objective

  A an objective is a short-term goal state that generates a drive
  module that is active until its goal is achieved.  The command
  arbitrator gives objective modules priority over drive modules, but
  all modules are constrained by constraint modules.

\end{itemize}

\subsubsection{Aversion Module}

An Aversion module is a behavior module that runs throughout the life
of its conataining agent and represents a state that an agent should
constantly avoid.  It is a constraint in the sense that, in certain
staes, a constraint module will identify actions that should *not* be
executed.


\subsubsection{Drive Module}

A Drive module is a behavior module that runs throughout the life of
its conataining agent and represents a state that an agent should
constantly seek.

\subsubsection{Knowledge-Based Modules}

In a knowledge-based module, the decision making and adaptation
mechanisms are coded explicitly in the form of rules, state machines,
or other explicitly programmed logic.  If a knowledge-based module is
adaptive, this adaptivity must be programmed by hand by the agent
author.  Because adaptivity is not "free," and thus not typically done
for knowledge-based modules, we distinguish between (possibly but not
usuallly adaptive) knowledge-based modules, and adaptive modules which
are always adaptive because the adaptivity is built-in.

\subsection{Drama Management Integration}

Activating and deactivating modules. Injecting new goals.

\subsubsection{Objective}

A an objective is a short-term goal state that generates a Drive module
that is active until its goal is acheived.

\subsection{Integrating Hierarchical Reinforcement Learning}

\subsection{Independent Language}\label{sec:conclusion-full-language}
