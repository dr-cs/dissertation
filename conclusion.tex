\chapter{Conclusion}\label{ch:conclusion}

\section{Review of Major Contributions}

This dissertation has reported on two primary contributions: a command arbitration algorithm for robust modular reinforcement learning, and a domain-specific language that supports modular reinforcement learning, AFABL. Our new command arbitration algorithm solves a problem that existed in previous approaches to modular reinforcement learning, namely, that reward scales for all modules had to be comparable. Supporting modules with incomparable rewards allows modules to be written separately, which supports modular agent software engineering, towards which AFABL is a first step. We demonstrated the benefits of language-integrated reinforcement learning in a study of programmers' solutions to agent programming tasks using AFABL compared to solutions using a traditional programming language. AFABL agents are easier to write, are expressed in less complex code, and have more readily reused components than agents written in traditional programming languages. In the remainder of this chapter we discuss some implications of AFABL (as a representative first step in language-integrated modular reinforcement learning), limitations of the current work, and directions for future work.

\section{Limitations of Current Work}

% The work presented here is promising, but focused in scope.

\subsection{Reward Authoring}

As many other researchers have noted, reward authoring is not straightforward for programmers not trained in reinforcement learning. Study participants spent much of their AFABL writing time trying out different reward structures in an effort to improve their agents' performance. Although we provided documentation with hints on how to author reward structures, writing good reward functions is too opaque for most programmers. In the next section we discuss a possible improvement to AFABL which would relieve programmers from writing the reward functions of modules.

\subsection{Training}

Using any reinforcement learning-based programming system requires the availability of a simulation environment to train the learning modules before being used ``in production.'' Using an untrained reinforcement learning agent and accepting that it will perform poorly until it learns is not practical because reinforcement learning algorithms typically require hundreds or thousands of iterations to reach an acceptable level of performance. Separate modules with local state abstractions and reward functions help speed up training, but finding good factorizations into modules is a potentially steep burden to place on the programmer for larger agents.

\subsection{Host Language Limitations}

Writing AFABL agents is writing Scala code, so AFABL programmers must have at least basic competence with Scala and the Scala tool chain. Since it was outside the scope of the present work, we did not try to determine how much of the Scala tooling can be hidden or automated for AFABL programmers. We required study participants to use a recent version of IntelliJ IDEA and provided a pre-configured Scala/AFABL project and an IntelliJ plug-in to automate the time tracking and submission process. Still, several participants had trouble running the study code smoothly, as is often the case with development tools. Many study participants who did not participate in a group session simply abandoned the study. We advertised the study to the Atlanta Scala Meet-up, a group of local software engineers either using Scala professionally or interested in learning. Approximately 15 Scala Meet-up members started the study and only one finished. Due to the number of individual software issues we needed to help participants solve -- differing operating systems, IntelliJ versions, etc -- we believe many of these dropouts were due to simple software setup issues.

In addition to Scala tooling issues, AFABL programmers must deal with the Scala programming language. For example, when a programmer makes a mistake in their code the error messages come from the Scala compiler and run-time system, not AFABL. Luckily, in the study few people had such issues with AFABL code itself. With more complex agents problems are more likely to occur, and the programmer may be faced with the famous complexity of the Scala type system. The AFABL programmer who is not also a competent Scala programmer has little hope of debugging non-trivial errors.

\section{Directions for Future Work}


\subsection{Refined Module Types}

AFABL currently supports a narrow definition of an agent: a behaving entity with a set of states that must constantly be pursued or avoided. In reinforcement learning these kinds of modules are called continuing tasks, as opposed to episodic goals. Previous versions of AFABL supported a greater set of features but we removed them to focus on AFABL's core for the purpose of this work. With a cleaner core AFABL we could re-implement some of these features and more, which we discuss below. The following features would provide a richer set of agent modeling tools for programmers.

\paragraph{Drives}

A Drive is a behavior module that runs throughout the life of its containing agent and represents a state that an agent should constantly seek.

\paragraph{Aversions}

An Aversion module is a behavior module that runs throughout the life of its containing agent and represents a state that an agent should constantly avoid.  It is a constraint in the sense that, in certain states, a constraint module will identify actions that should *not* be executed.

\paragraph{Objectives}

An objective is a short-term goal state that generates a drive module that is active until its goal is achieved.  The command arbitrator gives objective modules priority over drive modules, but all modules are constrained by constraint modules.

\paragraph{Tasks}

A task is a temporally-extended action, a "mini-policy" that achieves a subgoal.  Tasks are equivalent to subtasks (MaxQ), abstract machines (PHAM), or options from hierarchical reinforcement learning. Tasks could be manually authored, or algorithms from hierarchical reinforcement learning could be integrated into AFABL.

\subsection{Simplified Syntax}

The features listed above may make it possible to automatically author reward functions for modules. For example, the Bunny agent for Task 2 from Chapter \ref{ch:afabl-study} could be written with Drives and Aversions as shown in Figure \ref{fig:simplified-afabl}


\begin{figure}[!h]
\begin{center}

\begin{lstlisting}[language=Scala]
  val afablBunny2 = AfablAgent(

    world = bunnyWorld,

    drives = Drives(state: BunnyState) {
      (state.bunny == state.food),
      (state.bunny == state.mate)
    },

    aversions = Aversions(state: BunnyState) {
      (state.bunny == state.wolf)
    }

    agentLevelReward = (state: BunnyState) => {
      if (state.bunny == state.wolf) 0.0
      else if (state.bunny == state.food) 1.0
      else if (state.bunny == state.mate) 1.0
      else 0.5
    }
  )
\end{lstlisting}

\caption{Simplified AFABL syntax with drives and aversions.}
\end{center}
\label{fig:simplified-afabl}
\end{figure}


Instead of writing code to specify modules, the programmer specifies states that are to be constantly sought or avoided -- expressed as state predicates -- and the modules are derived from them automatically. Note also that this proposed syntax does not include state abstraction functions in modules because they could be derived automatically from the states that are to be sought or avoided.

\subsubsection{Drama Manager Support}

The features discussed above would go along way toward supporting drama managers for intelligent interactive narratives. In addition, a drama manager would need to be able to activate and deactivate modules and inject new objectives to support particular story goals.

\subsection{General Agent Architecture}

The current version of AFABL focuses on integrated reinforcement learning but could easily be extended to support integrated intelligence, that is, mixing of agent modules that employ different kinds of AI algorithms. Because an AFABL agent performs command arbitration over modules that support a behavioral interface (providing an action given a state observation) as opposed to merging elements of reinforcement learners (like Q-values), the modules themselves can employ any mechanism to decide on actions given a state.  This information hiding means that AFABL agents could be composed of a mixture of modules that use many different kinds of AI, including statistical learning, rule-based reasoning, or (reactive) planning.  In this sense AFABL would be an integrated intelligence architecture.

\paragraph{Knowledge-Based Arbitrators}

In addition to the modules the arbitrator itself could employ different kinds of algorithms for command arbitration. A knowledge-based arbitrator could use hand-coded logic to decide from among the actions recommended by an agent's modules.  Simple arbitrators with few modules to arbitrate can often be coded quite simply as knowledge-based arbitrators.

\paragraph{Hierarchical Decomposition}

Because modules are themselves agents, modules can contain other modules and perform command arbitration over those modules just as the top-level agent does.  Agents can thus be decomposed recursively into behavioral subsystems.  This recursive behavior module decomposition would provide the agent designer with great flexibility.  Recursive module composition is somewhat similar to the levels of competence in Brooks's subsumption architecture with an important difference: the internal workings of modules are never altered externally.  Modules are treated as black-boxes.  Command arbitration accomplishes the same result that output suppression does in classic subsumption.

\subsection{Independent (Non-Embedded) Language}\label{sec:conclusion-full-language}

Finally, once the additions to the language are integrated into the internal DSL and studied and refined sufficiently, an external DSL could be considered. Although an external DSL is far more work to implement, the benefits could justify the cost. A stand-alone version of AFABL would have its own set of development tools, report agent-oriented error messages to the user, and potentially run faster than equivalent internal DSL code.
