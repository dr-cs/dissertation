42026664,5,5,4,5,5,"I didn't have to put so much thought into the mechanics of moving and distance calculation, and didn't have to think about priorities.",4,All I had to do was add a 3rd module which was very much like FindFood.
32262970,5,5,5,5,5,"The choice of going towards to food or avoiding the wolf is tricky. Should you only avoid the wolf when it's adjacent? 2 Squares away? what exactly does it mean to avoid? Answering these questions is hard, but expressing the rewards and punishments for getting food and meeting the wolf is easy. AFABL did not require me to express these domain-specific problems, instead it learned based on the rewards and costs what policy is optimal.",5,"Same arguments as 1, but now there's the mate. I actually found that I could treat the mate exactly as a food source by adding another module very similar to the food module. The problem then became how to maximize the total reward by playing around with the module-level rewards and the agent-level rewards. If it were very important to find good reward values, one could run some optimization program on the rewards. It's worth mentioning that if I were faced with this problem, I would feel more comfortable running an optimization algorithm than writing my own domain-specific agent behavior. AFABL would pair nicely with a solver that optimizes on rewards."
54627009,4,5,4,5,4,"No need to explicitly compare locations, calculate distances or balance separate goals.",5,The design of independent modules made adding an additional module/goal trivial.
72486637,5,5,5,5,5,AFABL,5,AFABL
79564448,3,5,4,2,4,"Easier because it only asks for important parts of the specific domain, more difficult as I don't know how different goals interact and I don't know why my end score was lower in AFABL or what changes improve a score.",5,Both were pretty easy to add 1 more goal. But in scala I copied and pasted. In AFABL it was really neat to just import modules from the other file with no effort to integrate them.
11696648,4,4,5,5,3,I took some time to learn the basics and setup requirements for AFABL,5,It was much easier to add a module to AFABL than to add features in Scala2
45373694,4,4,4,4,4,"Less code, can just copy/paste for module",4,"Less code, can just copy/paste for module"
34303219,5,5,5,5,4,"While learning AFABL had some overhead for Task 1, being able to program in terms of rewards and punishments was much more intuitive than coding an algorithm from scratch that may or may not be correct.",5,Being able to just add in another module and tack it onto the agent with AFABL was much easier and more elegant than having to go in and modify existing methods and logic in scala. Adding the addtional functionality with AFABL was much more convenient in this respect.
44618213,4,3,3,2,2,"I've written agents similar to my scala agent in the past, so it took much less new thought to develop it, whereas the AFABL agent was conceptually and structurally very different from anything I've written before, and were therefore harder to approach.",5,"After understanding AFABL to some degree, it was quite easy to modify my existing agent to the new task and thereby receive a good score."
64949660,4,5,5,5,5,"The scala code was hard to work with because we were manually writing distance finding code. With AFABL, the rewards system was far more abstract. I felt like I could tweak the code much more easily to get the result I wanted in AFABL than scala",5,"It was easier to manage the three competing goals of the mate, the food, and avoiding the wolf in AFABL, since everything was rewards based. In scala I was doing really hack-ey distance calculations to determine which goal to go for, wheras AFABL abstracted that data out"
30394944,4,3,3,3,2,The concepts to learn were more abstract.,3,The concepts to learn were more abstract. The tasks were similar enough where the pros and cons were similar.
23333103,5,5,5,5,5,"AFABL eliminated the need to handle calculations such as distance, and tuning the model to prioritize a task was a simple as increasing a number.",5,"Extending the model using pure scala required me to modify the decision logic, but adding a behavior in AFABL only required me to add a new behavior module."
999999,4,4,4,4,4,Less thinking about logic and more about the reinforcement learning,4,Same as before
30487393,4,5,5,4,4,"Very straightforward syntax particularly with the provided documentation it was easy to make small tweaks in order to allow it to fit to the specific problem. Additionally, writing the tasks in scala required a lot of unnecessary checking and lots of stacked if/else to figure out locations. Using AFABL was way more straightforward.",4,Same as for Task 1.
79979378,5,5,5,5,5,"You simply define success and failure scenarios; there's no dealing with weighting different values and that makes it MUCH easier to work with, as a programmer.",5,"You can much more clearly see the similarities between Task 1 and Task 2 in the AFABL version, for one thing. Second, it doesn't require modifying existing code nearly as much as the plain Scala version does. It's a delight to use, and as a programmer at a startup, I would much rather work with this format over what I have to do to work with AWS' Machine Learning program."
45474051,4,5,5,5,5,The easy of associated varying magnitude positive and negative scores with re-enforcement learning criteria was way easier than coming up with a specific algorithm for the learning environment.,5,"Like Task 1, Task 2 was made far easier to prototype with higher accuracy thanks to using parameters as opposed to code for agent learning."
14376916,5,5,4,5,5,There was no need to define the movement algorithm,4,"Too many cases to work on in Scala, hard to figure out whether mating or eating takes importance, takes far less time with AFABL."