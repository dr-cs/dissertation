#+TITLE:     Integrating Reinforcement Learning into a Programming Language
#+AUTHOR:    Chris Simpkins
#+EMAIL:
#+DATE:      1 February 2017
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS: H:2 toc:nil num:t
#+BEAMER_FRAME_LEVEL: 2
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [smaller,aspectratio=1610]
#+LaTeX_HEADER: \usepackage{verbatim, multicol, tabularx,color}
#+LaTeX_HEADER: \usepackage{amsmath,amsthm, amssymb, latexsym, listings, qtree}
#+LaTeX_HEADER: \usepackage{algorithm}
#+LaTeX_HEADER: \usepackage[noend]{algpseudocode}
#+LaTeX_HEADER: \usepackage{biblatex}
#+LaTeX_HEADER: \bibliography{references}
#+LaTeX_HEADER: \lstset{frame=tb, aboveskip=1mm, belowskip=0mm, showstringspaces=false, columns=flexible, basicstyle={\tiny\ttfamily}, frame=single, breaklines=true, breakatwhitespace=true}
#+LaTeX_HEADER: \lstdefinelanguage{scala}{morekeywords={abstract,case,catch,class,def, do,else,extends,false,final,finally, for,if,implicit,import,match,mixin, new,null,object,override,package, private,protected,requires,return,sealed, super,this,throw,trait,true,try, type,val,var,while,with,yield}, otherkeywords={=>,<-,<\%,<:,>:,\#,@}, sensitive=true, morecomment=[l]{//}, morecomment=[n]{/*}{*/}, morestring=[b]", morestring=[b]', morestring=[b]"""}
#+LaTeX_HEADER: \setbeamertemplate{footline}[frame number]

* Integrating Reinforcement Learning into a Programming Language

** My Thesis

(1) Modular reinforcement learning modules can be made composable and

- (in the current state of the art in modular reinforcement learning, modules are not composable)

(2) integrating modular reinforcement learning into a programming language improves reuse and reduces complexity of intelligent agent code.

- (Intelligent agent code written in traditional programming languages tends to be complex and not amenable to reuse.)

** Reinforcement Learning

A behavioral agent acts in a world typically modeled as a Markov Decision Process (MDP):

\begin{equation}
\langle S, A, T(s, a, s'), R(s) \rangle
\end{equation}

where

- $S$ is a set of states,
- $A$ is a set of actions, and
- $T(s, a, s')$ is a transition function which gives the probably that executing action $a$ in state $s$ will result in $s'$.
- $R(s)$ is a reward function that indicates the one-step reward for arriving in state $s$

In typical reinforcement learning problems, there is a state with higher reward that represents a "goal." A reinforcement learning agent learns a policy, $\pi(s)$ that maps states to actions such that following the policy maximizes long-term expected reward (Sutton and Barto 1998).

** Q-Learning (Watkins 1998)

In typical learning scenarios we don't have the MDP, so we learn a direct mapping from states to actions, an action-value function, a Q-function.

1. $Q \gets$ random initial values
2. For each episode
   1. $s \gets$ world.initialState()
   2. While $s$ is not terminal
      1. $a$ $\gets$ $\epsilon-$greedy action for $s$ from $\pi$ derived from $Q$
      2. Execute $a$, observe effects $r$ and $s'$
      3. $Q(s, a) \gets Q(s, a) + \alpha [R(s) + \gamma \max_{a'} Q(s', a') - Q(s, a)]$
      4. $s \gets s'$

For the Sarsa variant of Q-learning, save $a$ in $a'$ and replace the update in Step 2.2.3 with

\begin{equation}
Q(s, a) \leftarrow Q(s, a) + \alpha [R(s) + \gamma Q(s', a') - Q(s, a))]
\end{equation}

Sarsa is *on-policy* because the temporal difference used in the Q-update is based on the policy being followed by the agent during learning. The standard Q-learning update is *off-policy* because the Q-update is based on the best known next action.

** Curse of Dimensionality

The state space grows exponentially in the number of state features. As an example, consider the $5 \times 5$ grid of the bunny world.

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1.5in
[[file:../bunny.png]]
#+END_CENTER

The bunny, food, and wolf can be in one of 25 possible locations.

- Single task, e.g., bunny reaching a single location, state space is 25.
- Add task of avoiding a wolf, state space grows to $25^2 = 625$.
- Add task of finding food, state space grows to $25^3 = 15625$.

** Modular Reinforcement Learning

(a.k.a. multiple-goal or multi-objective reinforcement learning)

A reinforcement learning agent composed of modules. Instead of one monolithic reward and Q-function, there's a separate reward signal and Q-function for each module.

- Learning agent, $M$, decomposed into $n$ modules, $M=\{M_i\}_{i=1}^n$
- Each $M_i = (S_i,A,R_i)$ - shared action set, distinct state spaces and rewards

The agent uses the modules' separate Q-functions to decide a single policy for the agent.

- MRL bunny has two modules: FindFood and AvoidWolf.

Instead of a single monolithic state space of $25^3 = 15625$ states, two separate state spaces with $25^2 = 625$ states each (bunny plus wolf and bunny plus food).


** MRL State of the Art

Merged Q-function assuming additive rewards (Russel and Zimdars, 2003; Sprague and Ballard, 2003; Roijers, et. al. 2013):

#+BEGIN_CENTER
$Q_{joint}(s, a) = \sum Q_i(s, a)$
#+END_CENTER

Roijers, Vamplew, et. al. call this a *scalarization* of the separate reward and Q-functions.

- Important assumption: rewards are additive. We'll show how this results in non-composable modules.

** Bunny World

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1.5in
[[file:../bunny.png]]
#+END_CENTER

- Bunny must constantly find food and avoid the wolf.
- Bunny actions are move up, down, left, or right.
- Food is static, wolf moves towards bunny every other time step.
- When bunny finds finds food it consumes the food and new food appears elsewhere in the grid world.
- When bunny meets wolf it is eaten and ``respawns'' elsewhere.

MRL bunny agent: FindFood module and AvoidWolf module

** Comparable and Incomparable Rewards

Comparable Rewards:
- FindFood rewards: +1 for finding food, -0.1 otherwise
- AvoidWolf rewards: -1 for meeting wolf, 0.1 otherwise

Incomparable Rewards:
- FindFood rewards: +10 for finding food, -1 otherwise
- AvoidWolf rewards: -1 for meeting wolf, 0.1 otherwise

By *incomparable* we mean rewards have different scales, e.g., FindFood rewards are 10x greater magnitude than AvoidWolf's.


** Problem 1: Current MRL is not composable

Y-axis shows a score, not a reward, since rewards may be incomparable. Score is +1 every time bunny eats, 0 when wolf finds bunny and 0.5 for surviving another step but not eating.

#+BEGIN_CENTER
#+ATTR_LaTeX: :height 2.5in
[[file:../gm-bunny-wolf.png]]
#+END_CENTER

GM-Sarsa (Sprague and Ballard 2003)/Q-Decomposition (Russell and Zimdars 2003) degrades when rewards are not comparable.

** Composite GM-Sarsa Q-values with Comparable Rewards

Simplified example:

#+ATTR_LaTeX: :align |p{1em}|p{1em}|p{1em}|p{1em}|p{1em}|
|---+---+---+---+---|
|   |   | B | W | F |
|---+---+---+---+---|

Given the comparable rewards listed previously, the composite Q-values for the Right and Left actions would be

\begin{align*}
Q(s, Left) &= Q_{FindFood}(s, Left) + Q_{AvoidWolf}(s, Left)\\
           &= 0.72 + 0.95 = 1.67
\end{align*}

\begin{align*}
Q(s, Right) &= Q_{FindFood}(s, Right) + Q_{AvoidWolf}(s, Right)\\
            &= 0.8 - 0.4 = 0.4
\end{align*}

Given these composite Q-values the next action decided by GM-Sarsa would be Left, which is correct because it avoids getting eaten by the wolf.

** Composite GM-Sarsa Q-values with Incomparable Rewards

#+ATTR_LaTeX: :align |p{1em}|p{1em}|p{1em}|p{1em}|p{1em}|
|---+---+---+---+---|
|   |   | B | W | F |
|---+---+---+---+---|

Given the incomparable rewards listed previously resulting in a scaled Q-function for FindFood and the same AvoidWolf values as above, the composite Q-values would be

\begin{align*}
Q(s, Left) &= Q_{FindFood}(s, Left) + Q_{AvoidWolf}(s, Left)\\
           &= 6.2 + 0.95 = 7.15
\end{align*}

and

\begin{align*}
Q(s, Right) &= Q_{FindFood}(s, Right) + Q_{AvoidWolf}(s, Right)\\
            &= 8.0 - 0.4 = 7.6
\end{align*}

and the bunny would move right and get eaten by the wolf.

- Scaling FindFood's rewards causes FindFood to dominate action selection.

** MRL Reformulation

An agent is a list of modules, as before, and an arbitrator. Formally:

- An action set $A$ for the agent as a whole, shared by each module,
- A set of reinforcement learning modules, $M$
- A state abstraction function, $moduleState_i$ for each module $m_i$ (optional, but useful)
- A reward function, $R_i(s)$ for each module $m_i$
- A command arbitrator that chooses one of the modules' action choice as the agent's single action

** Ideal Arbitration is Impossible (Bhat, et. al. 2006)

Note that our MRL reformulation is voting: each module votes on an action, the arbitrator's job is to chose one action. A "fair" voting scheme would have the following properties:

- **Universality**: the ability to handle any possible set of modules.

- **Unanimity**: guarantee that if every module prefers action A, action A will be selected.

- **Independence of Irrelevant Alternatives**: each module's preference for actions A and B are independent of the availability of any other action C. This property prevents any particular module from affecting the global action choice by dishonestly reporting its own preference ordering.

- **Scale Invariance**: ability to scale any module's Q-values without affecting the arbitrator's choice.  This is the crucial property that allows separately authored modules with incomparable reward signals.

- **Non-Dictatorship**: no module gets its way all the time.

According to social choice theory (Arrow, 1963, Roberts 1980), if $|A|\geq 3$, then there does not exist an arbitration function that satisfies each of the properties listed above.

We must relax one of the requirements ...

** Arbi-Q

Arbi-Q is a command arbitration algorithm that uses a Q-learning algorithm to learn a policy mapping states to modules. In a given state, a particular module choose's the agent's single action. In summary:

- Command arbitrator has its own reward function, $R_{CA}(s)$
- Action set $A_{CA}$ that represents choosing a module in a given state
- Each module gets single unweighted vote for an action in each state
- Command arbitrator uses Q-learning to learn a policy mapping states to modules

Command arbitrator is a "benevolent dictator", that is, a  module that "get's its way" all the time. Other desirable properties will still hold.


** Solution 1: Arbi-Q Results

Y-axis shows a score, not a reward, since rewards may be incomparable. Score is +1 every time bunny eats, 0 when wolf finds bunny and 0.5 for surviving another step but not eating.

#+BEGIN_CENTER
#+ATTR_LaTeX: :height 2.5in
[[file:../arbiq-bunny-wolf.png]]
#+END_CENTER

Rewards may be incomparable between modules -- no degradation of performance. So separately authored modules can be composed in the same agent.

** The Catch - Arbitrator Reward Function

Arbi-Q achieves composability by decoupling module reward scales at the cost of requiring a separately authored reward function.

- Not obvious how to author arbitrator reward
  - Represents greater good, "why" find food, "why" avoid wolf -- to live longer
  - Think of as score in a video game
- If the reward function uses the original world state, then its state space is the same size as a monolithic reinforcement learner
  - In this case the savings in learning speed compared to a monolithic reinforcement learner is the ratio of module/agent actions to the number of modules.
    - Q-table of a monolithic reinforcement learner would have $|S| \times |A|$ entries
    - Q-table of arbitrator has $|S| \times n$ entries where $n$ is the number of modules

** Software Engineering

Two important issues in software engineering:

- Reuse - reusing existing artifacts in the construction of new software
  - Reusable artifacts include code, concepts, patterns
- Complexity - the effort required to understand or modify a piece of code
  - McCabe's cyclomatic complexity: the number of paths through the control flow graph of a program
  - Simple calcluation: number of decision structures + 1 (McCabe 1960)

Problem 2 of my thesis statement: intelligent agent code tends to be complex and not amenable to reuse.

** Domain-Specific Languages

A domain-specific language (DSL) is a language that provides constructs and semantics tailored to a specific problem domain.

- Well-known example: SQL

#+BEGIN_SRC scala
select name, creator from language where paradigm='functional'
#+END_SRC

versus

#+BEGIN_SRC scala
List<String, String> funcLangs = new ArrayList<>();
for (Record lang: langs) {
    if (lang.paragigm().equals("functional") {
        funcLangs.add(new Tuple(lang.name(), lang.creator()));
    }
}
#+END_SRC

SQL provides reusable language constructs and semantics that map directly to relational data model, resulting in far less complex code.

** AFABL

AFABL (A Friendly Adaptive Behavior Language) is a domain-specific language for writing adaptive intelligent agents.

- Improves reuse of problem domain concepts and application-specific code through domain-specific language
- Reduces complexity with a declarative syntax

Declarative agent code is transformed into modular reinforcement learning agents by the DSL.

** AFABL Worlds

Every AFABL agent and module operates in a particular world defined by

- states,

#+BEGIN_SRC scala
case class Location(x: Int, y: Int)

case class BunnyState(
  bunny: Location,
  wolf: Location,
  food: Location
)
#+END_SRC


- actions

#+BEGIN_SRC scala
object BunnyAction extends Enumeration {
  val Up = Value("^")
  val Down = Value("v")
  val Left = Value("<")
  val Right = Value(">")
}
#+END_SRC

- and world dynamics

#+BEGIN_SRC scala
abstract class World[S, A] {
  def init(): S
  def resetAgent(): S
  def states: Seq[S]
  def actions: Seq[A]
  def act(action: A): S
}
#+END_SRC

** AFABL Modules

A module is a reinforcement learner that can be composed with other modules in an ~AfablAgent~. Each module has

- a world in which it can act,
- a state abstraction function which defines the subset of the world the module cares about, and
- a module reward function that shapes the module's learned behavior.

#+BEGIN_SRC scala
case class FindFoodState(bunny: Location, food: Location)

val findFood = AfablModule(
  world = new BunnyWorld,

  stateAbstraction = (worldState: BunnyState) => {
    FindFoodState(worldState.bunny, worldState.food)
  },

  moduleReward = (moduleState: FindFoodState) => {
    if (moduleState.bunny == moduleState.food) 1.0
    else -0.1
  }
)
#+END_SRC

** AFABL Agents

An AFABL agent is an agent that acts in a particular world, is composed of independent behavior modules pursuing their own continuing goals, and has a central command arbitrator that uses an agent level reward function to learn when it should listen to each module.

#+BEGIN_SRC scala
val bunny = AfablAgent(
  world = new BunnyWorld,

  modules = Seq(findFood, avoidWolf),

  agentLevelReward = (state: BunnyState) => {
    if (state.bunny == state.wolf) 0.0
    else if (state.bunny == state.food) 1.0
    else 0.5
  }
)
#+END_SRC

Claim: Using AFABL results in agent code that is less complex and more amenable to reuse than equivalent agent code in a traditional language.

** AFABL Programmer Study World

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1.5in
[[file:../bunny.png]]
#+END_CENTER

- Bunny must constantly find food and avoid the wolf.
- Bunny actions are move up, down, left, or right.
- Food is static, wolf moves one step towards bunny every other time step.
- When bunny finds finds food it consumes the food and new food appears elsewhere in the grid world.
- When bunny meets wolf it is eaten and ``respawns'' elsewhere.

** AFABL Programmer Study Tasks

Write agents for the following tasks in Scala and AFABL.

1. Task 1: write a bunny agent that finds as much food as possible and avoids the wolf as much as possible.
2. Task 2: same as Task 1, but add a mate that acts like the food (static, reappears after mating). Bunny must find food, avoid wolf, and mate as much as possible.

Study mechanics screen-cast ...

** Problem 2.1: Agent Code in Traditional Language is Complex

Typical Scala Agent for Task 1. Look at all the action selection logic!

#+BEGIN_SRC scala
class ScalaBunny1 extends Agent[BunnyState, BunnyAction.Value]
    with Task1Scorer {

  def getAction(state: BunnyState, shouldExplore: Boolean = false) = {
    if (wolfNearFood(state))
      moveAwayFromWolf(state)
    else
      moveTowardFood(state)
   }
  def wolfNearFood(state: BunnyState) = {
    val wolfToFood = sqrt(pow(state.food.x - state.wolf.x, 2) +
                          pow(state.food.y - state.wolf.y, 2))
    val bunnyToFood = sqrt(pow(state.food.x - state.bunny.x, 2) +
                           pow(state.food.y - state.bunny.y, 2))
    wolfToFood < bunnyToFood
  }
  def moveTowardFood(state: BunnyState) = {
    if (state.food.x > state.bunny.x)
      BunnyAction.Right
    else if (state.food.x < state.bunny.x)
      BunnyAction.Left
    else if (state.food.y < state.bunny.y)
      BunnyAction.Up
    else
      BunnyAction.Down
  }
  def moveAwayFromWolf(state: BunnyState) = {
    if (state.wolf.x < state.bunny.x)
      BunnyAction.Right
    else if (state.wolf.x > state.bunny.x)
      BunnyAction.Left
    else if (state.wolf.y > state.bunny.y)
      BunnyAction.Up
    else
      BunnyAction.Down
  }
}
#+END_SRC

** Problem 2.2: Agent Code in Traditional Language is Not Amenable to Reuse

Typical Scala Agent for Task 2. Reuse required refactoring helper methods.

#+BEGIN_SRC scala
class ScalaBunny2 extends Agent[BunnyState, BunnyAction.Value]
    with Task2Scorer {

  def getAction(state: BunnyState, shouldExplore: Boolean = false) = {
    if ((distance(state.wolf, state.food) < distance(state.food, state.bunny))
      || distance(state.wolf, state.mate) < distance(state.mate, state.bunny))
      moveAwayFromWolf(state)
    else if (distance(state.bunny, state.food) < distance(state.bunny, state.mate))
      moveToward(state.bunny, state.food)
    else
      moveToward(state.bunny, state.mate)
  }
  def distance(a: Location, b: Location) = {
    sqrt(pow(a.x - b.x, 2) + pow(a.y - b.y, 2))
  }
  def moveToward(from: Location, to: Location) = {
    if (to.x > from.x)
      BunnyAction.Right
    else if (to.x < from.x)
      BunnyAction.Left
    else if (to.y > from.y)
      BunnyAction.Up
    else
      BunnyAction.Down
  }
  def moveAwayFromWolf(state: BunnyState) = {
    if (state.wolf.x < state.bunny.x)
      BunnyAction.Right
    else if (state.wolf.x > state.bunny.x)
      BunnyAction.Left
    else if (state.wolf.y > state.bunny.y)
      BunnyAction.Up
    else
      BunnyAction.Down
  }
}
#+END_SRC

** Solution 2.1: AFABL is Less Complex

Typical Task 1 submission: code is declarative -- specify what, not how. Less complex, easier to maintain (McCabe 1976)

#+BEGIN_SRC scala
  case class FindFoodState(bunny: Location, food: Location)
  val findFood = AfablModule(
    world = bunnyWorld,
    stateAbstraction = (worldState: BunnyState) => {
      FindFoodState(worldState.bunny, worldState.food)
    },
    moduleReward = (moduleState: FindFoodState) => {
      if (moduleState.bunny == moduleState.food) 1.0
      else -0.1
    }
  )

  case class AvoidWolfState(bunny: Location, wolf: Location)
  val avoidWolf = AfablModule(
    world = bunnyWorld,
    stateAbstraction = (worldState: BunnyState) => {
      AvoidWolfState(worldState.bunny, worldState.wolf)
    },
    moduleReward = (moduleState: AvoidWolfState) => {
      if (moduleState.bunny == moduleState.wolf) -0.1
      else 0.1
    }
  )

  val afablBunny1 = AfablAgent(

    world = bunnyWorld,

    modules = Seq(findFood, avoidWolf),

    agentLevelReward = (state: BunnyState) => {
      if (state.bunny == state.wolf) 0.0
      else if (state.bunny == state.food) 1.0
      else 0.5
    }
  )
#+END_SRC

** Solution 2.2: AFABL Facilitates Reuse

Typical Task 2 submission: modules from Task 1 are directly reusable. DSL provides domain-specific reuse opportunities.

#+BEGIN_SRC scala
  case class FindMateState(bunny: Location, mate: Location)
  val findMate = AfablModule(
    world = bunnyWorld,
    stateAbstraction = (state: BunnyState) => {
      FindMateState(state.bunny, state.mate)
    },
    moduleReward = (state: FindMateState) => {
      if (state.bunny == state.mate) 1.0
      else -0.1
    }
  )

  // Your solution must assign your AFABL bunny agent for Task 2 to
  // the val afablBuny2.
  val afablBunny2 = AfablAgent(

    world = bunnyWorld,

    modules = Seq(AfablTask1.findFood, AfablTask1.avoidWolf, findMate),

    agentLevelReward = (state: BunnyState) => {
      if (state.bunny == state.wolf) 0.0
      else if (state.bunny == state.food) 1.0
      else if (state.bunny == state.mate) 1.0
      else 0.5
    }
  )
#+END_SRC


** AFABL Programmer Study Results

n=16

Task 1: FindFood + AvoidWolf

|                       |      Scala Mean |      AFABL Mean | p-value |
|-----------------------+-----------------+-----------------+---------|
| Lines of Code         |            39.3 |            31.2 |    0.23 |
| Time                  | 1511.5s (25.2m) | 1780.9s (29.7m) |    0.49 |
| Cyclomatic complexity |            10.8 |             5.3 |    0.01 |

Task 2: FindFood + AvoidWolf + FindMate

|                       |     Scala Mean |     AFABL Mean | p-value |
|-----------------------+----------------+----------------+---------|
| Lines of Code         |           41.7 |           39.2 |    0.58 |
| Time               | 797.8s (13.3m) | 626.7s (10.4m) |    0.55 |
| Cyclomatic complexity |           11.3 |            8.2 |    0.04 |


Solution 2: AFABL agents were less complex than Scala agents for the same tasks, and appeared to be easier to adapt to Task 2. AFABL time results likely affected by need to read documentation.


** AFABL Programmer Study Questionnaire Responses

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1.5in
[[file:../reflection-q3-results.png]]

#+ATTR_LATEX: :height 1.5in
[[file:../reflection-q4-results.png]]
#+END_CENTER


** AFABL Programmer Study Participant Reflections

#+BEGIN_QUOTE
While learning AFABL had some overhead for Task 1, being able to program in terms of rewards and punishments was much more intuitive than coding an algorithm from scratch that may or may not be correct.
#+END_QUOTE

#+BEGIN_QUOTE
Being able to just add in another module and tack it onto the agent with AFABL was much easier and more elegant than having to go in and modify existing methods and logic in scala. Adding the additional functionality with AFABL was much more convenient in this respect.
#+END_QUOTE

#+BEGIN_QUOTE
You can much more clearly see the similarities between Task 1 and Task 2 in the AFABL version, for one thing. Second, it doesn't require modifying existing code nearly as much as the plain Scala version does. It's a delight to use, and as a programmer at a startup, I would much rather work with this format over what I have to do to work with AWS' Machine Learning program.
#+END_QUOTE

** Application: Personality Modeling

- Assign numeric values to each of several traits, or personality dimensions.
- Popular example: Five-Factor Model: Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism (McCrae & Paul T. Costa 2008).
- Many different trait theories, but key idea is common: model personality as multiple numeric scales.

Basic idea: trait-theoretic personality models can be translated into reinforcement learning framework.

#+BEGIN_CENTER

| Psychology          | Reinforcement Learning |
|---------------------+------------------------|
| Trait               | RL Module              |
| Valence             | Reward                 |
| Trait measure/score | Weight on RL module    |

#+END_CENTER

** Atkinson's Ring Toss Experiment

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1.5in
[[file:ring-toss-186x186.jpg]]
#+END_CENTER

- Atkinson and Litwin (1960) studied Achievement Motivation and Fear of Failure.
- 49 Students classified as high or low in both Achievement Motivation and Test Anxiety (Fear of Failure).
- Each student played a ring toss game at one of 15 distances from ring.

** Simulating Atkinson's Experiment With AFABL Agents

A motivated student with low fear of failure:

#+BEGIN_SRC scala
val achievementMotivation = AfablModule(
  world = RingTossWorld,
  moduleReward = (state: RingTossState) => state match {
    case OneFootLine => 1,
    case TwoFootLine => 2,
    ...
    case FifteenFootLine => 15
  }
)
val testAnxiety = AfablModule(
  world = RingTossWorld,
  moduleReward = (state: RingTossState) => state match {
    case OneFootLine => 15,
    case TwoFootLine => 14,
    ...
    case FifteenFootLine => 1
  }
)
val motivatedStudent = GmAgent(
  world = RingTossWorld,

  // Sequence of pairs where the second element of each pair
  // is the weight of the pair, corresponding to the personality
  // trait measure
  modules = Seq((achievementMotivation, 8), (testAnxiety, 2))
}
#+END_SRC

** Results of Virtual Atkinson Reproduction

- Ran 10 virtual replications of Atkinson's experiment.
- Generated data similar to Atkinson's human subjects

#+ATTR_LATEX: :width 2.5in
[[file:../atkinson.png]] [[file:../iccm.png]]

Just a proof of concept, but promising

** Limitations of AFABL

- Need for simulation environment to (pre)train agents
- Reward authoring is not straightforward for programmers not trained in reinforcement learning
- Host language limitations

** Opportunities

- Simplified syntax removing most reward authoring
- Integration of hierarchical reinforcement learning (Precup 1998, Dietterich 1998, Parr 1998, Andre 2000)
- More reusable concepts in DSL: drives, aversions, objectives, tasks
- Drama manager features

#+BEGIN_SRC scala
  val afablBunny2 = AfablAgent(

    world = bunnyWorld,

    drives = Drives(state: BunnyState) {
      (state.bunny == state.food),
      (state.bunny == state.mate)
    },

    aversions = Aversions(state: BunnyState) {
      (state.bunny == state.wolf)
    }

    agentLevelReward = (state: BunnyState) => {
      if (state.bunny == state.wolf) 0.0
      else if (state.bunny == state.food) 1.0
      else if (state.bunny == state.mate) 1.0
      else 0.5
    }
  )
#+END_SRC

** Contributions

1. A command arbitration algorithm for modular reinforcement learning -- Arbi-Q -- that enables composability by decoupling the reward scales of reinforcement learning modules, and
2.  a Scala-embedded domain-specific language -- AFABL (A Friendly Adaptive Behavior Language) -- that integrates modular reinforcement learning in a way that allows programmers to use reinforcement learning without knowing much about reinforcement learning algorithms.


** The Dissertator

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1in
[[file:dissertator.jpg]]
#+END_CENTER

Published

- Towards Adaptive Programming: Integrating Reinforcement Learning into a Programming Language, OOPSLA Onward! 2008
- Deriving Behavior from Personality: A Reinforcement Learning Approach, ICCM 2010

To be published:

- Command Arbitration for Robust Modular Reinforcement Learning, ICML 2017 (Deadline: 24 Feb 2017)
- A Friendly Adaptive Behavior Language, OOPSLA 2017 (Deadline: 17 Apr 2017)

** Backup Slides

- Q-value calculation details

** FindFood with Comparable Reward Scales

With comparable rewards the Q-value of moving right for FindFood would be (we use deterministic state transition dynamics here for simplicity)

\begin{align*}
Q(s, Right) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
              &= -0.1 + 0.9 (1.0)\\
              &= 0.8
\end{align*}

because the max next action would find the food.

The value of moving left would be

\begin{align*}
Q(s, Left) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
             &= -0.1 + 0.9 (0.8)\\
             &= 0.72
\end{align*}

because the max next action would be Right, to get closer to the food.

** AvoidWolf with Comparable Reward Scales

With comparable rewards the Q-value of moving right for AvoidWolf would be

\begin{align*}
Q(s, Right) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
            &= 0.5 + 0.9 (-1.0)\\
            &= -0.4
\end{align*}

because the next state meets the wolf.

The value of moving left would be

\begin{align*}
Q(s, Left) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
             &= 0.5 + 0.9 (0.5)\\
             &= 0.95
\end{align*}

because the max next action would again avoid the wolf.


** Composite GM-Sarsa Q-values with Comparable Rewards

Given the module Q-values above, the composite Q-values for the Right and Left actions would be

\begin{align*}
Q(s, Left) &= Q_{FindFood}(s, Left) + Q_{AvoidWolf}(s, Left)\\
           &= 0.72 + 0.95 = 1.67
\end{align*}

\begin{align*}
Q(s, Right) &= Q_{FindFood}(s, Right) + Q_{AvoidWolf}(s, Right)\\
            &= 0.8 - 0.4 = 0.4
\end{align*}

Given these composite Q-values the next action decided by GM-Sarsa would be Left, which is correct because it avoids getting eaten by the wolf.

** FindFood with Incomparable Reward Scales

If we scale the FindFood module's rewards by 10, the Q-values for moving right and left would be

\begin{align*}
Q(s, Right) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
              &= -1.0 + 0.9 (10.0)\\
              &= 8.0
\end{align*}

and

\begin{align*}
Q(s, Left) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
             &= -1.0 + 0.9 (8.0)\\
             &= 6.2
\end{align*}

** Composite GM-Sarsa Q-values with Incomparable Rewards

Using the same AvoidWolf values as above and the scaled FindFood Q-values using incomparable rewards the composite Q-values would be

\begin{align*}
Q(s, Left) &= Q_{FindFood}(s, Left) + Q_{AvoidWolf}(s, Left)\\
           &= 6.2 + 0.95 = 7.15
\end{align*}

and

\begin{align*}
Q(s, Right) &= Q_{FindFood}(s, Right) + Q_{AvoidWolf}(s, Right)\\
            &= 8.0 - 0.4 = 7.6
\end{align*}

and the bunny would move right and get eaten by the wolf.

This example demonstrates how scaling the FindFood module's rewards causes the preferences of FindFood to dominate action selection, resulting in the bunny getting eaten and not getting to the food.
