#+TITLE:     Integrating Reinforcement Learning into a Programming Language
#+AUTHOR:    Chris Simpkins
#+EMAIL:
#+DATE:      2 February 2017
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS: H:2 toc:nil num:t
#+BEAMER_FRAME_LEVEL: 2
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [smaller]
#+LaTeX_HEADER: \usepackage{verbatim, multicol, tabularx,}
#+LaTeX_HEADER: \usepackage{amsmath,amsthm, amssymb, latexsym, listings, qtree}
#+LaTeX_HEADER: \lstset{frame=tb, aboveskip=1mm, belowskip=0mm, showstringspaces=false, columns=flexible, basicstyle={\ttfamily}, numbers=left, frame=single, breaklines=true, breakatwhitespace=true}
#+LaTeX_HEADER: \setbeamertemplate{footline}[frame number]

* Integrating Reinforcement Learning into a Programming Language

** My Thesis

(1)Modular reinforcement learning modules can be made composable and
(2) integrating modular reinformcement learning into a programming language improves code reuse and reduces the complexity of certain kinds of intelligent agents.

** Contributions

This dissertation makes two contributions:

1. A command arbitration algorithm for modular reinforcement learning that enables composability by decoupling the reward scales of reinforcement learning modules, and
2.  a Scala-embedded domain-specific language -- AFABL (A Friendly Adaptive Behavior Language) -- that integrates modular reinforcement learning in a way that allows programmers to use reinforcement learning without knowing much about reinforcement learning algorithms.

** Modular Reinforcement Learning

- Learning agent, $M$, decomposed into $n$ modules, $M=\{M_i\}_{i=1}^n$
- Each $M_i = (S_i,A,R_i)$ - shared action set, distinct state spaces and rewards

State of the art: merged Q-function assuming additive rewards (Russel and Zimdars, 2003; Sprague and Ballard, 2003):

#+BEGIN_CENTER
$Q_{joint}(s, a) = \sum Q_i(s, a)$
#+END_CENTER

** Problem 1: Current MRL is not composable

[[file:../gm-bunny-wolf.png]]

Rewards must be comparable between modules, otherwise performance degrades.

** Why GM-Sarsa/Q-Decomposition Degrades

Simplified bunny world example (B is for bunny, F is for food, W is for wolf):

#+ATTR_LaTeX: :align |p{1em}|p{1em}|p{1em}|p{1em}|p{1em}|
|---+---+---+---+---|
|   |   | B | W | F |
|---+---+---+---+---|
|   |   |   |   |   |
|---+---+---+---+---|
|   |   |   |   |   |
|---+---+---+---+---|
|   |   |   |   |   |
|---+---+---+---+---|
|   |   |   |   |   |
|---+---+---+---+---|

- FindFood rewards: +1 for finding food, -0.1 otherwise
- AvoidWolf rewards: -1 for meeting wolf, 0.1 otherwise

** FindFood with Comparable Reward Scales

With comparable rewards the Q-value of moving right for FindFood would be (we use deterministic state transition dynamics here for simplicity)

\begin{align*}
Q(s, Right) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
              &= -0.1 + 0.9 (1.0)\\
              &= 0.8
\end{align*}

because the max next action would find the food.

The value of moving left would be

\begin{align*}
Q(s, Left) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
             &= -0.1 + 0.9 (0.8)\\
             &= 0.72
\end{align*}

because the max next action would be Right, to get closer to the food.

** AvoidWolf with Comparable Reward Scales

With comparable rewards the Q-value of moving right for AvoidWolf would be

\begin{align*}
Q(s, Right) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
            &= 0.5 + 0.9 (-1.0)\\
            &= -0.4
\end{align*}

because the next state meets the wolf.

The value of moving left would be

\begin{align*}
Q(s, Left) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
             &= 0.5 + 0.9 (0.5)\\
             &= 0.95
\end{align*}

because the max next action would again avoid the wolf.

** Composite GM-Sarsa Q-values with Comparable Rewards

Given the module Q-values above, the composite Q-values for the Right and Left actions would be

\begin{align*}
Q(s, Left) &= Q_{FindFood}(s, Left) + Q_{AvoidWolf}(s, Left)\\
           &= 0.72 + 0.95 = 1.67
\end{align*}

\begin{align*}
Q(s, Right) &= Q_{FindFood}(s, Right) + Q_{AvoidWolf}(s, Right)\\
            &= 0.8 - 0.4 = 0.4
\end{align*}

Given these composite Q-values the next action decided by GM-Sarsa would be Left, which is correct because it avoids getting eaten by the wolf.

** FindFood with Incomparable Reward Scales

If we scale the FindFood module's rewards by 10, the Q-values for moving right and left would be

\begin{align*}
Q(s, Right) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
              &= -1.0 + 0.9 (10.0)\\
              &= 8.0
\end{align*}

and

\begin{align*}
Q(s, Left) &= R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')\\
             &= -1.0 + 0.9 (8.0)\\
             &= 6.2
\end{align*}

** Composite GM-Sarsa Q-values with Incomparable Rewards

Using the same AvoidWolf values as above and the scaled FindFood Q-values using incomparable rewards the composite Q-values would be

\begin{align*}
Q(s, Left) &= Q_{FindFood}(s, Left) + Q_{AvoidWolf}(s, Left)\\
           &= 6.2 + 0.95 = 7.15
\end{align*}

and

\begin{align*}
Q(s, Right) &= Q_{FindFood}(s, Right) + Q_{AvoidWolf}(s, Right)\\
            &= 8.0 - 0.4 = 7.6
\end{align*}

and the bunny would move right and get eaten by the wolf.

This example demonstrates how scaling the FindFood module's rewards causes the preferences of FindFood to dominate action selection, resulting in the bunny getting eaten and not getting to the food.

** MRL Reformulation

An agent is an arbitrator plus a list of modules. Formally:

- An action set $A$ for the agent as a whole, shared by each module,
- A set of reinforcement learning modules, $M$
- A state abstraction function, $moduleState_i$ for each module $m_i$
- A reward function, $R_i(s)$ for each module $m_i$
- An arbitrator with:

  - A reward function for the command arbitrator, $R_{CA}(s)$,
  - Action set $A_{CA}$ that represents choosing a module in a given state

** Arbi-Q

Arbi-Q is a command arbitration algorithm that uses a Q-learning algorithm to learn a policy mapping states to modules.

- Each module gets single unweighted vote for an action in each state
- Command arbitrator chooses one of the modules' actions

Command arbitrator is a "benevolent dictator"


** Arbi-Q Results

[[file:../arbiq-bunny-wolf.png]]

Rewards may be incomparable between modules -- no degradation of performance. So separately authored modules can be composed in the same agent, Problem 1 of my thesis statement.

** Software Engineering

Two important issues in software engineering:

- Reuse - makes software engineering more efficient and code more maintainable
- Complexity - the effort required to understand or modify a piece of code

Problem 2 of my thesis statement: intelligent agent code tends to be complex and not amenable to reuse.

** AFABL

AFABL (A Friendly Adaptive Behavior Language) is a domain-specific langauge for ariting adaptive intelligent agents.

- Improves reuse of problem domain concepts and application-specific code through domain-specific language
- Reduces complexity with a declarative syntax

Declarative agent code is transformed into modular reinforcement learning agents by the DSL.

** AFABL Example

#+BEGIN_SRC scala
  case class FindFoodState(bunny: Location, food: Location)
  val findFood = AfablModule(
    world = bunnyWorld,
    stateAbstraction = (worldState: BunnyState) => {
      FindFoodState(worldState.bunny, worldState.food)
    },
    moduleReward = (moduleState: FindFoodState) => {
      if (moduleState.bunny == moduleState.food) 1.0
      else -0.1
    }
  )
  // avoidWolf similar to findFood
  val afablBunny1 = AfablAgent(
    world = bunnyWorld,
    modules = Seq(findFood, avoidWolf),
    agentLevelReward = (state: BunnyState) => {
      if (state.bunny == state.wolf) 0.0
      else if (state.bunny == state.food) 1.0
      else 0.5
    }
  )
#+END_SRC

** AFABL Programmer Study World

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1.5in
[[file:../bunny.png]]
#+END_CENTER

- Bunny must constantly find food and avoid the wolf.
- Bunny actions are move up, down, left, or right.
- Food is static, wolf moves one step towards bunny every other time step.
- When bunny finds finds food it consumes the food and new food appears elsewhere in the grid world.
- When bunny meets wolf it is eaten and ``respawns'' elsewhere.

** AFABL Programmer Study Tasks

Write agents for the following tasks in Scala and AFABL.

1. Task 1: write a bunny agent that finds as much food as possible and avoids the wolf as much as possible.
2. Task 2: same as Task 1, but add a mate that acts like the food (static, reappears after mating). Bunny ust find food, avoid wolf, and mate as much as possible.

** AFABL Programmer Study Results

Task 1:

|                       | Scala Mean | AFABL Mean | p-value |
|-----------------------+------------+------------+---------|
| Lines of Code         |        0.0 |        0.0 |     0.0 |
| Time                  |        0.0 |        0.0 |     0.0 |
| Cyclomatic complexity |        0.0 |        0.0 |     0.0 |
| Performance           |        0.0 |        0.0 |     0.0 |

Task 2:

|                       | Scala Mean | AFABL Mean | p-value |
|-----------------------+------------+------------+---------|
| Lines of Code         |        0.0 |        0.0 |     0.0 |
| Time                  |        0.0 |        0.0 |     0.0 |
| Cyclomatic complexity |        0.0 |        0.0 |     0.0 |
| Performance           |        0.0 |        0.0 |     0.0 |


Problem 2 of my thesis: AFABL agents were shorter, took less time to write, and were less complex than Scala agents for the same tasks.

** AFABL Programmer Study Questionnaire Responses

*** Questionnaire responses                                    :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_env: ignoreheading
    :BEAMER_col: 0.5
    :END:

    #+ATTR_LATEX: :height 1.25in
    [[file:../reflection-q2-results.png]]

    #+ATTR_LATEX: :height 1.25in
    [[file:../reflection-q3-results.png]]

*** Questionnaire responses                                    :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_env: ignoreheading
    :BEAMER_col: 0.5
    :END:

    #+ATTR_LATEX: :height 1.25in
    [[file:../reflection-q4-results.png]]

    #+ATTR_LATEX: :height 1.25in
    [[file:../reflection-q5-results.png]]


** AFABL Programmer Study Participant Reflections

#+BEGIN_QUOTE
While learning AFABL had some overhead for Task 1, being able to program in terms of rewards and punishments was much more intuitive than coding an algorithm from scratch that may or may not be correct.
#+END_QUOTE

#+BEGIN_QUOTE
Being able to just add in another module and tack it onto the agent with AFABL was much easier and more elegant than having to go in and modify existing methods and logic in scala. Adding the addtional functionality with AFABL was much more convenient in this respect.
#+END_QUOTE

#+BEGIN_QUOTE
You can much more clearly see the similarities between Task 1 and Task 2 in the AFABL version, for one thing. Second, it doesn't require modifying existing code nearly as much as the plain Scala version does. It's a delight to use, and as a programmer at a startup, I would much rather work with this format over what I have to do to work with AWS' Machine Learning program.
#+END_QUOTE

** Application: Personality Modeling

Basic idea: trait-theoretic personality models can be translated into reinforcement learning framework.

#+BEGIN_CENTER

| Psychology          | Reinforcement Learning |
|---------------------+------------------------|
| Trait               | RL Module              |
| Valence             | Reward                 |
| Trait measure/score | Weight on RL module    |

#+END_CENTER

** Atkinson's Ring Toss Experiment

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1.5in
[[file:ring-toss-186x186.jpg]]
#+END_CENTER

- Atkinson and Litwin studied Achievement Motivation and Fear of Failure.
- 49 Students classified as high or low in both Achievement Motivation and Test Anxiety (Fear of Failure).
- Each student played a ring toss game at one of 15 distances from ring.

** Simulating Atkinson's Experiment With AFABL Agents

#+BEGIN_SRC scala
val achievementMotivation = AfablModule(
  world = RingTossWorld,
  moduleReward = (state: RingTossState) => state match {
    case OneFootLine => 1,
    case TwoFootLine => 2,
    ...
    case FifteenFootLine => 15
  }
)
val testAnxiety = AfablModule(
  world = RingTossWorld,
  moduleReward = (state: RingTossState) => state match {
    case OneFootLine => 15,
    case TwoFootLine => 14,
    ...
    case FifteenFootLine => 1
  }
)
#+END_SRC


- Ran 10 virtual replications of Atkinson's experiment.
- Generated data similar to Atkinson's human subjects

** Limitations of AFABL

- Need for simulation environment to (pre)train agents
- Reward authoring is not straightforward for programmers not trained in reinforcement learning
- Host language limitations

** Opportunities

- Simplified syntax removing most reward authoring
- Integration of HRL
- Drives, aversions, objectives, tasks
- Drama manager features

** Review of Contributions

Thesis: (1)Modular reinforcement learning modules can be made composable and
(2) integrating modular reinformcement learning into a programming language improves code reuse and reduces the complexity of certain kinds of intelligent agents.

- Arbi-Q makes modular reinforcement learning composable by decoupling the reward scales of modules
- AFABL integrates MRL/Arbi-Q, which facilitates reuse and reduces complexity


** The Dissertator

#+BEGIN_CENTER
#+ATTR_LATEX: :height 1in
[[file:dissertator.jpg]]
#+END_CENTER

Published

- Towards Adaptive Programming: Integrating Reinforcement Learning into a Programming Language, OOPSLA Onward! 2008
- Deriving Behavior from Personality: A Reinforcement Learning Approach, ICCM 2010

To be published:

- Command Arbitration for Robust Modular Reinforcement Learning, ICML 2017 (Deadline: 24 Feb 2017)
- A Friendly Adaptive Behavior Language, OOPSLA 2017 (Deadline: 17 Apr 2017)
