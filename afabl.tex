\chapter{AFABL: A Friendly Adaptive Behavior Language}\label{ch:afabl}

AFABL is an internal domain-specific language (DSL) shallowly embedded in the Scala programming language. In this chapter we explain why we chose to implement AFABL as a Scala-embedded DSL, present the basic elements of AFABL with examples, and report the results of a programmer study which confirm and quantify the usefulness of integrating reinforcement learning into a programming language.

\section{Why an embedded DSL?}

We chose to implement AFABL as a shallowly embedded domain-specific language because of the exploratory nature of this research. Our goal at this point is to confirm our expectation that integrating reinforcement learning is useful to programmers writing agents, to explore the nature of this integration, and to get qualitative feedback from programmers on their experience using a language that integrates reinforcement learning. Writing a full language with its own lexical and syntactic structure, internal representations, and tools (interpreters, compilers, linkers, etc.) would distract from the core questions we are trying to answer. As we discuss in Section \ref{sec:conclusion-full-language}, creating a full independent language is a direction for future research which will be guided by the results we present here, for which an embedded DSL is sufficient.

\section{Why Scala?}

Hosting DSLs is a primary design goal of the Scala programming language. Scala is an expressive and concise language which already enables the expression of domain models with little syntactic baggage. By employing just a few Scala language features that are designed for writing expressive and convenient libraries, we can create a DSL that Scala programmers will find familiar and non-Scala programmers can use to encode adaptive agents. Indeed, the nature of Scala's syntax and language features is such that many Scala libraries that are not explicitly labeled as DSLs qualify as shallow embedded DSLs. \added[id=v3]{In Scala a DSL is simply a library which takes advantage of Scala's language features and idioms that makes code using the library look like a custom language.}

\section{AFABL Concepts}

AFABL is a language for encoding adaptive (intelligent) agents. Before we present the AFABL language, we review basic adaptive agent concepts that can be encoded in AFABL.

%% \subsection{Terminology}

%% \begin{itemize}

%% \item State. A state is a configuration of all objects and agents in a world. These configurations can include object locations and orientations, mental or emotional dispositions of agents, or indications of the occurrence of events (e.g., agent X just got shot).

%% \item Perception, or Percept. A subset of a world state that is perceived by an agent (or module).

%% \item Action. An action is a one-shot state manipulation that can be executed by an agent.  Sometimes called "primitives," actions acquire greater meaning when they are executed (possibly in sequences) by behavior modules to achieve a goal or satisfy a constraint.


%% \item World. Sometimes called an "environment," a world is a container for all the things an agent can perceive and act upon, and possibly hidden things.  Worlds are represented by states that specify the configuration of all the things in a world at a given point in time.

%% \item World Dynamics. The dynamics of a world specifies the state transitions that are made in response to the execution of actions.  World dynamics may be deterministic or stochastic.  Markov Decision Processes are often used to represent world dynamics.

%% \item Behavior Module. A behavior module is a self-contained agent component that recommends an action (response) for a given state perception (stimulus).  Anything that produces a policy (a mapping from states to actions and tasks) is a behavior module.  A behavior module is sometimes called a "subagent" in modular reinforcement learning.

%% \item Agent. An agent is an entity that acts under its own control, perceiving the state of its world and executing actions in response to these perceived states.

%% \item Intelligent Agent. An intelligent agent is an agent that chooses its actions to achieve goals or satisfy constraints.

%% \item Adaptive Agent. An adaptive agent is an intelligent agent that can automatically adapt its behaviors to different worlds (that is, choose different actions sequences to achieve goals in worlds with different dynamics), or adapt its behaviors at run-time to worlds in which the dynamics change.

%% \item Modular Agent. A modular agent is an agent that consists of multiple behavior modules and performs command arbitration to decide which module's preferred action to execute in a given state.

%% \item Command Arbitration. Command arbitration is the act of deciding, for a given state, which behavior module should choose the action for the agent.


%% \end{itemize}

\subsection{Agent Architecture}

An AFABL agent is a behavioral agent that is composed of reusable behavior modules.  \replaced[id=v3]{We use behavioral agent in the sense common in agent programming literature \cite{mateas2004a-behavior} -- an agent receives a percept from the environment and executes an action in response.}{By "behavioral agent" we mean that the agent executes an action in response to a stimulus, represented by a state observation.}  Each behavior module is itself an agent that has a preferred action for each state.  An AFABL agent performs command arbitration to choose one of its modules' recommended actions for each state.  The behavior modules recommend actions in each state, and the arbitrator chooses which module to "listen to" in each state.  \deleted[id=v3]{The AFABL agent architecture is a subsumption architecture \cite{brooks1986a-robust}}

\subsection{Behavior Modules}

Behavior modules, sometimes called subagents in the modular reinforcement learning literature, are agents that are meant to be combined to form larger agents.  Behavior modules are similar to the layers of Brooks's subsumption architecture with an important difference: autonomy.  The internal working of a behavior module is never altered externally.  A behavior module defines a state abstraction that converts the state observation it is given to a (possibly) simpler state that is used internally for decision making and learning.  \added[id=v3]{Module state abstractions that are simpler (contain fewer features) than the global state help to speed the convergence of the underlying reinforcement learning algorithms.} The decision making and adaptation mechanisms inside a module remain completely under the module's control.  Interaction with the module consists entirely of reporting a state observation to the module, asking the module for an action, and reporting to the module the effect of executing an action.

\subsection{Adaptive Modules}

An adaptive module employs learning algorithms under the hood to achieve automatic adaptivity.  By adaptive we mean two things: (1) adaptation to new worlds, and (2) run-time adaptation.  A module that is programmed to work for worlds with a given state representation will work with any world that provides the same state representation, even if the dynamics of the worlds differ.  An adaptive module need simply be retrained for the new world.  Once an adaptive module is running in an active agent, the module may continue to tune its internal learning models as the agent acts in the world, providing for run-time adaptation.

\subsection{Command Arbitrators}

A command arbitrator takes as input the state of the world and the action preferences of a set of modules, and selects one of the modules or actions to be executed by the agent.

\section{The AFABL Language}

AFABL is a \replaced[id=v3]{DSL implemented as a library}{framework/DSL} in the Scala programming language \replaced[id=v3]{designed for writing}{for implementing} adaptive agents. An AFABL agent operates in a world, is composed of one or more modules, and has an agent level reward function that it uses to learn a command arbitration policy.

\subsection{Worlds}

Every AFABL module and agent is designed to operate in a world. A world defines the states, actions and state transition dynamics for a given \replaced[id=v3]{set of states and actions}{state and action types}. Details are discussed below.

\subsubsection{States}

The states of a world can be represented with any kind of Scala class. Case classes are good for representing states because of their concise syntax and built-in equality methods. Figure \ref{fig:bunny-state-code} shows a case class for a state with three state variables: the locations of a bunny, wolf, and food.

\begin{figure}[!h]
\begin{center}

\begin{lstlisting}[language=Scala]
case class Location(x: Int, y: Int)

case class BunnyState(
  bunny: Location,
  wolf: Location,
  food: Location
)
\end{lstlisting}

\caption{Scala code to represent states in the bunny world.}
\end{center}
\label{fig:bunny-state-code}
\end{figure}

\subsubsection{Actions}

Actions are represented by objects which can be instances of any class. As with states, case classes make a good choice for implementing actions. Figure \ref{fig:bunny-action-code} shows actions for the bunny world implemented as a Scala enumeration.

\begin{figure}[!h]
\begin{center}

\begin{lstlisting}[language=Scala]
object BunnyAction extends Enumeration {
  val Up = Value("^")
  val Down = Value("v")
  val Left = Value("<")
  val Right = Value(">")
}
\end{lstlisting}

\caption{Scala code to represent the actions that the bunny agent can take in the bunny world.}
\end{center}
\label{fig:bunny-action-code}
\end{figure}

\subsubsection{World Dynamics}

An agent executes actions in a world, and those actions potentially change the state of the world. Having defined Scala representations for states and actions, we can define a world. Figure \ref{fig:world-code} shows the abstract class which defines the basic interface of world objects, which are instances of subclasses of {\tt World}. As we discuss in Sections \ref{sec:afabl-modules} and \ref{sec:afabl-agents}, all modules and agents are defined to act in a particular instance of a world. As with states and actions, world representations make no advanced use of the Scala programming language.

\begin{figure}[!h]
\begin{center}
\small
\begin{lstlisting}[language=Scala]
abstract class World[S, A] {
  def init(): S
  def resetAgent(): S
  def states: Seq[S]
  def actions: Seq[A]
  def act(action: A): S
}
\end{lstlisting}
\normalsize
\caption{The abstract superclass of all world classes for AFABL agents.}
\end{center}
\label{fig:world-code}
\end{figure}

%% \begin{figure}[p]
%% \begin{center}
%% \small
%% \begin{lstlisting}[language=Scala]
%% /** A stateful object representing world dynamics.
%%   */
%% abstract class World[S, A] {

%%   /** Initialize the world to a start state.
%%     */
%%   def init(): S

%%   /** Move agent to a start state, according to the rules of the
%%     * implementing world. Useful for continuing worlds where there is
%%     * no terminal state and the agent "respawns" after dying.
%%     */
%%   def resetAgent(): S

%%   /** All the states of this world. Necessary for reinforcement learning
%%     * agents.
%%     */
%%   def states: Seq[S]

%%   /** All the actions an agent can execute in this world.  For
%%     * simplicity this includes all the actions, and the actions that
%%     * aren't available in a given state simply have no
%%     * effect. Necessary for reinforcement learning agents.
%%     */
%%   def actions: Seq[A]

%%   /** Execute action in the world, resulting in a (possibly) new state.
%%    */
%%   def act(action: A): S
%% }
%% \end{lstlisting}
%% \normalsize
%% \caption{The abstract superclass of all world classes for AFABL agents.}
%% \end{center}
%% \label{fig:world-code}
%% \end{figure}


Figure \ref{fig:bunny-world-code} shows some of the code for the Bunny World.

\begin{figure}[!h]
\begin{center}
\small
\begin{lstlisting}[language=Scala]
class BunnyWorld(val width: Int = 5, val height: Int = 5)
  extends World[BunnyState, BunnyAction.Value] with LazyLogging {

  // Initialize the world state
  var state = init()

  // In Scala, defs can be overridden with vals
  val states = {
    // Calculate every possible combination of locations for the
    // bunny, wolf, and food
  }

  // This line returns all the values of the BunnyAction enumeration
  val actions = BunnyAction.values.toSeq

  def init(): BunnyState = {
    // Calculate initial locations for the bunny, wolf, and food.
    //
  }

  def resetAgent(): BunnyState = {
    // "Respawn" the bunny at a new location, update the world state
    // and return the new state
  }

  def act(intendedAction: BunnyAction.Value): BunnyState = {
    // Code to calculate the actual action due to uncertainty in the
    // environment and update the state of the world based on the
    // actual action.
  }
  // Helper functions ...
}
\end{lstlisting}
\normalsize
\caption{Parts of the bunny world class showing important aspects of the implementation of the {\tt World} abstract class.}
\end{center}
\label{fig:bunny-world-code}
\end{figure}


\subsection{Modules}\label{sec:afabl-modules}

\begin{figure}[!h]
\begin{center}

\begin{lstlisting}[language=Scala]
case class FindFoodState(bunny: Location, food: Location)

val findFood = AfablModule(
  world = new BunnyWorld,

  stateAbstraction = (worldState: BunnyState) => {
    FindFoodState(worldState.bunny, worldState.food)
  },

  moduleReward = (moduleState: FindFoodState) => {
    if (moduleState.bunny == moduleState.food) 1.0
    else -0.1
  }
)
\end{lstlisting}

\caption{AFABL code for a module that represents the goal of constantly finding food.}
\end{center}
\label{fig:find-food-code}
\end{figure}

Figure \ref{fig:find-food-code} shows the complete code for an AFABL implementation of a behavior module that represents the goal of finding food. First is the definition of a case class, {\tt FindFoodState}, to represent the state abstraction for FindFood modules. {\tt FindFoodState} includes only two of the three state variables in the bunny world.

\begin{center}
\begin{lstlisting}[language=Scala,frame=none]
case class FindFoodState(bunny: Location, food: Location)
\end{lstlisting}
\end{center}

We store a reference to an {\tt AfablModule} for FindFood in {\tt findFood}.

\begin{center}
\begin{lstlisting}[language=Scala,frame=none]
val findFood = AfablModule(
\end{lstlisting}
\end{center}

The {\tt AfablModule} factory method takes three arguments: an instance of a {\tt World} that the module can act and learn in, a {\tt stateAbstraction} function, and a {\tt moduleReward} function.

The first argument to {\tt AfablModule} is the world:

\begin{center}
\begin{lstlisting}[language=Scala,frame=none]
world = new BunnyWorld
\end{lstlisting}
\end{center}

The {\tt world} and {\tt =} are optional, but if included must be verbatim, i.e., considered part of the AFABL language.

The second argument is a state abstraction function that takes a world-state object as a parameter and returns an instance of our state abstraction class:

\begin{center}
\begin{lstlisting}[language=Scala,frame=none]
stateAbstraction = (worldState: BunnyState) => {
  FindFoodState(worldState.bunny, worldState.food)
}
\end{lstlisting}
\end{center}

The {\tt stateAbstraction} and {\tt =} are optional, but if included should be considered part of the AFABL language. {\tt worldState} is a user-chosen name, {\tt BunnyState} must match the state type defined for the world in which the module and agent operate, in this case it is the first type parameter to {\tt World} in the {\tt BunnyWorld} code in Figure \ref{fig:bunny-world-code}. The last expression in the body of the {\tt stateAbstraction} function must be an instance of a module state, in this case {\tt FindFoodState}.

The third and final argument to the {\tt AfablModule} factory method is a module reward function that takes an instance of our state abstraction class and returns the reward this module receives for being in that state:

\begin{center}
\begin{lstlisting}[language=Scala,frame=none]
moduleReward = (moduleState: FindFoodState) => {
  if (moduleState.bunny == moduleState.food) 1.0
  else -0.1
}
\end{lstlisting}
\end{center}

The {\tt moduleReward} and {\tt =} are optional, but if included should be considered part of the AFABL language. {\tt moduleState} is a user-chosen name, but the parameter type, {\tt FindFoodState} in this example, must match the return type of the {\tt stateAbstraction} function. The last expression in the body of the {\tt moduleReward} function must be a {\tt Double} value. In this case, which is typical, the body of the {\tt moduleReward} function is an {\tt if} expression which simply returns the reward based on state predicates. This example is another case where we could have implemented DSL-specific syntax, such as a list of predicates and values, but the syntactic overhead of Scala's {\tt if} expression is minimal and the code is crystal clear to any Scala programmer.

These three components -- world, state abstraction and module reward -- define a module specific learning problem on a subset of the world in which the module (and agent containing the module) may act. Internally, AFABL uses these components to instantiate a Sarsa learning algorithm using the algorithm parameters discussed in Chapter \ref{ch:arbiq}, but the programmer need not be aware of any details of reinforcement learning {\it algorithms}. The AFABL programmer need only be familiar with the reinforcement learning {\it problem}.

This module example shows the value of splitting the world dynamics from the agent module's reward function. We can think of the world and the agent independently. In essence, the definition of a full MDP is split across the definition of a world, and the definition of an agent that acts in that world.

Here we also begin to see syntactic conveniences afforded by the AFABL DSL. There are only two type annotations and one control structure (in the reward function). The rest of the types are inferred by Scala's type inferencer thanks to the way we wrote the factory method that creates {\tt AfablModule}s. It's worth noting that we could have refined the DSL to further strip the few Scala syntactic artifacts (like the {\tt if} statement and the anonymous functions for {\tt stateAbstraction} and {\tt moduleReward}) but the syntactic overhead is minimal and there is a tradeoff between writing specific DSL syntax and using Scala's built-in syntax directly. Creating unique syntax for a DSL imposes cognitive burden on programmers who are proficient in the host language. The benefit of the unique syntax must outweigh this cognitive burden. Here we hope to strike the right balance between convenient domain-specific syntax and familiarity to programmers. Thanks to Scala's already concise and expressive language and idioms, although this looks like a DSL there is no special syntax in this example. This code is a good example of shallow DSL embedding.

\subsection{Agents}\label{sec:afabl-agents}

An AFABL agent is an agent that acts in a particular world, is composed of independent behavior modules pursuing their own continuing goals, and has a central command arbitrator that uses an agent level reward function to learn when it should listen to each module. As the code in Figure \ref{fig:afabl-bunny-code} shows, an AFABL agent allows programmers to express these components concisely, with very little cognitive distance between the concepts that make up the agent and the code that represents them. As with modules, the {\tt AfablAgent} class uses features of the Scala programming language to make the syntax more convenient. For example, there is only one explicit type annotation in the AFABL bunny agent code in Figure \ref{fig:afabl-bunny-code}, but behind the scenes a carefully written factory method in the companion object allows Scala's static type inferencer to infer type parameters of the {\tt AfablAgent} constructor, return types for anonymous functions, and assign a concrete type value to a path-dependent abstract type variable. Figuring out all this stuff and wrestling with Scala's type checker directly is not easy. Writing an AFABL agent is easy.

\begin{figure}[!h]
\begin{center}

\begin{lstlisting}[language=Scala]
val bunny = AfablAgent(
  world = new BunnyWorld,

  modules = Seq(findFood, avoidWolf),

  agentLevelReward = (state: BunnyState) => {
    if (state.bunny == state.wolf) 0.0
    else if (state.bunny == state.food) 1.0
    else 0.5
  }
)
\end{lstlisting}

\caption{An AFABL agent that acts in a world, contains behavior modules, and has an agent level reward.}
\end{center}
\label{fig:afabl-bunny-code}
\end{figure}



\subsection{A Complete AFABL Bunny}

A complete bunny agent using the AFABL DSL is shown in Figure \ref{fig:afabl-bunny-code}. This code would typically fit in a single editor window and represents a tremendous amount of functionality. This agent pursues two goals simultaneously and prioritizes them based on the relative locations of the bunny, the food, and the wolf.

\begin{figure}[!h]
\begin{center}

\begin{lstlisting}[language=Scala]
val bunnyWorld = new BunnyWorld

case class FindFoodState(bunny: Location, food: Location)
val findFood = AfablModule(
  world = bunnyWorld,
  stateAbstraction = (worldState: BunnyState) => {
    FindFoodState(worldState.bunny, worldState.food)
  },
  moduleReward = (moduleState: FindFoodState) => {
    if (moduleState.bunny == moduleState.food) 1.0
    else -0.1
  }
)

case class AvoidWolfState(bunny: Location, wolf: Location)
val avoidWolf = AfablModule(
  world = bunnyWorld,
  stateAbstraction = (worldState: BunnyState) => {
    AvoidWolfState(worldState.bunny, worldState.wolf)
  },
  moduleReward = (moduleState: AvoidWolfState) => {
    if (moduleState.bunny == moduleState.wolf) -0.1
    else 0.1
  }
)

val bunny = AfablAgent(
  world = new BunnyWorld,
  modules = Seq(findFood, avoidWolf),
  agentLevelReward = (state: BunnyState) => {
    if (state.bunny == state.wolf) 0.0
    else if (state.bunny == state.food) 1.0
    else 0.5
  }
)
\end{lstlisting}

\caption{A complete bunny agent in the AFABL DSL. Code for the modules is repeated from previous figures to give a sense of the full quantity of code required to write an agent with two behavior modules.}
\end{center}
\label{fig:afabl-bunny-code}
\end{figure}

\section{Conclusion}

In the next Chapter we provide a quantitative evaluation of the benefits of integrating reinforcement learning into a programming language.
