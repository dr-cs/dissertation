\chapter{Background in Software Engineering}\label{ch:se}

Software engineering is the process of creating software that is correct, reliable, and maintainable. This chapter provides background in software engineering that relates to our work. In particular, we claim in the next chapter that AFABL provides two benefits that have been central issues in software engineering: AFABL facilitates reuse and reduces complexity. Here we discuss the issue of reuse in software engineering and relate reuse to domain-specific languages, then briefly discuss complexity measurement in software engineering. Finally, we close with a discussion of adaptive programming.

\section{Software Reuse}

Software reuse was identified as a primary tool in improving software engineering practice since the birth of the field of software engineering in 1968 \cite{mcilroy1968mass}. Software reuse means using an existing software artifact in a new software system, ideally without modifying the original artifact \cite{krueger1992a-software,frakes2005a-software}. Reusable artifacts may be source code libraries, components, programming languages, and application frameworks \cite{polancic2010a-an-empirical} as well as concepts such as software schemas, architectures, and design patterns. The benefits of software reuse seem obvious, and empirical studies have indeed shown that reuse reduces defect rates, reduces refactoring costs, and increases productivity \cite{basili1996a-how-reuse,mohagheghi2008a-an-empirical}. In the next chapter we also quantify the reduction in code complexity afforded by the AFABL DSL. DSLs, as we discuss below, are a particular kind of reusable artifact.

Krueger presents a useful framework for understanding and assessing reuse techniques. Of particular interest to our work, he discusses reuse techniques in terms of cognitive distance, which he defines as an intuitive measure of the effort required to use a reusable software artifact in the process of turning the concept of a software application into a working system. The smaller the cognitive distance between a reusable software artifact and the concept of the application program in which it is to be reused, the more successful the reuse. Thus, abstraction is crucial to software reuse, the higher the level of abstraction the better. Krueger proposes three techniques for minimizing the cognitive distance in reusable software artifacts: ``(1) using fixed and variable abstractions that are both succinct and expressive, (2) maximizing the hidden parts of the abstractions, and (3) using automated mappings from abstraction specifications to abstraction realizations'' \cite{krueger1992a-software}. Krueger was the first to recognize that high level languages such as C and Java are themselves examples of software reuse -- language constructs are abstraction specifications, assembly language or byte code are abstraction realizations. DSLs (or VHLLs -- Very High Level Languages -- as he called them) are also examples of software reuse that raise the level of abstraction even higher, offering abstraction specifications that are entities in some problem domain such as set theory or circuit design. In the next chapter we will analyze AFABL according to Krueger's framework.

Gacek argues for creating domain-specific reference architectures to facilitate reuse\cite{gacek1995a-exploiting}. A domain-specific application architecture identifies all of the components that comprise a software application for a particular domain and the interactions between the components. A domain-specific language can be seen as a domain-specific architecture, whose components are modeled as language abstractions. AFABL, in this sense, is a domain-specific architecture for agents with multiple continuing goals.

\subsection{Domain-Specific Languages}

A domain-specific language (DSL) is a language that provides constructs and semantics tailored to a specific problem domain. Specialized programming languages were already widespread when Landin proposed the first unified framework for designing domain specific languages in 1966 \cite{landin1966next}. In Landin's framework the design of a DSL consists of two independent parts: the written form of the language, and the kinds of abstractions that can be expressed in the language. Every language has an abstract syntax, axiomatization, and an underlying abstract machine. Today many DSLs are in widespread use for various application domains such as typesetting and manuscript preparation (\TeX and \LaTeX), circuit design (VHDL), web page authoring (HTML and CSS), data exchange (JSON and XML), and many more.

Perhaps the most successful DSL, with which every reasonably literate software engineer or computer scientist is familiar, is Structured Query Language (SQL) \cite{}. SQL was originally presented as SEQUEL in 1974 by Chamberlin and Boyce of IBM Research \cite{chamberlin1974sequel}. Today SQL is used in all significant relational databases and its ANSI/ISO standard is on its third version. SQL has succeeded so completely because it provides exactly the right abstractions and semantics for using relational databases.

Domain-specific languages provide two primary benefits in software engineering: improving programmer productivity and improving communication with domain experts \cite{fowler2011domain}. In Chapter \ref{ch:afabl} we show that AFABL improves programmer productivity by reducing the effort required to write agents and reducing the complexity of agent code. In Chapter \ref{ch:applications} we present an application of AFABL to the domain of personality modeling in psychology to demonstrate AFABL's usefulness as a tool for bridging specialist knowledge from a non-computing domain with computational models.

Hudak argued that a domain-specific language is the ``ultimate abstraction,'' providing abstractions and semantics tailored to a particular application domain, but that languages are typically difficult to implement and difficult to evolve as the domain is better understood and changes need to be made to the language \cite{hudak1996building}. To solve this problem Hudak argued for and demonstrated domain-specific {\it embedded} languages, that is, DSLs embedded in a general-purpose host language, inheriting the tooling, syntax, and semantics of the host language while adding domain-specific constructs. Hudak used Haskell and showed how higher-order typed languages were particularly well-suited for hosting DSELs \cite{hudak1998modular}. For AFABL we used Scala, which has similarities to Haskell, as we discuss in the next chapter.

Ward \cite{ward1994language} proposed Language Oriented Programming as a way of organizing software development. Instead of developing reusable libraries in a general purpose language the software engineer develops a formal domain-specific specification of the application, then implements this specification as a DSL. He calls the resulting software development process ``middle-out'' development, where the DSL is created first (the middle layer) then the DSL is implemented (the lower layer) and specific applications are developed using the DSL (the upper layer). He presents several examples of this approach, including \LaTeX \cite{lamport1986document} as a collection of \TeX \cite{knuth1984texbook} macros and Emacs \cite{stallman2014emacs}, which is essentially a Lisp interpreter with addressable memory buffers -- what users think of as the editor is actually implemented as a collection of Emacs Lisp functions, making Emacs infinitely extensible.
Neighbors was the first to propose {\it domain languages} specifically for the purpose of reuse \cite{neighbors1984draco}. Lorenz and colleagues tie together the concept of reuse with Language Oriented Programming in software engineering \cite{lorenz2011a-code}, comparing the effort and benefits of implementing and using internal versus external DSLs. They performed a case study in which they created an external DSL (using a language workbench system called Meta-Programming System MPS \cite{dmitriev2004language}) and internal DSL using the experimental LOP language Cedalion \cite{rosenan2010designing} for the same task (calculator software product line). They found that both approaches achieved their code reuse goals but, while both DSLs took similar effort to use, the external DSL took four times longer to implement. They concluded that internal DSLs should be favored over external DSLs for most software development projects. Rosenan argues that host languages themselves can be designed with internal DSL creation in mind -- a kind of ``Language-Oriented Programming language'' -- and presents a LOP language as a proof of concept \cite{rosenan2010designing}. As we discuss in the next chapter, while not explicitly advertised as an LOP language, hosting DSLs has been a design goal of the Scala language that we use to host AFABL.


%% \cite{taha2008domain-specific}

%% \cite{dmitriev2004a-language}


%% \cite{mitchell1993on-abstraction}

%% \cite{simpkins2008towards}


%% \cite{zang2007towards}


%% \cite{mernik2005when}


\section{Software Complexity}

A second goal of AFABL is reducing the complexity of agent code. For our purposes we define complexity as a measure of the effort required to understand, modify, or test a piece of code. Many kinds of complexity measures have been proposed, including function point analysis \cite{albrecht1979measuring}, Halstead's ``software science'' \cite{halstead1977elements}, information flow \cite{henry1981software} and many others. But perhaps the most widespread complexity measure is McCabe's cyclomatic complexity \cite{mccabe1976complexity,mccabe1989design}. Roughly speaking, the McCabe cyclomatic complexity number is a measure of the number of unique paths through a program.

Though there are critiques of cyclomatic complexity
\cite{gill1991cyclomatic} and proposed modifications \cite{weyuker1988evaluating}, McCabe's cyclomatic complexity measure is still in widespread use and has been shown to be a useful and valid measure. Curtis and colleagues found that both Halstead and McCabe complexity metrics correlate with psychological complexity of code, especially for less experienced programmers \cite{curtis1979measuring}. Finally, McCabe's cyclomatic complexity number has a simple method of calculation which makes it particularly appealing. We explain McCabe's cyclomatic complexity in the next chapter.

\section{Adaptive Programming}

By adaptive software we refer to the notion used in the machine learning community: software that learns to adapt to its environment during run-time, not software that is written to be easily changed by modifying the source code and recompiling.  In particular, we use Peter Norvig's definition of adaptive software:

\begin{quote}
Adaptive software uses available information about changes in its
environment to improve its behavior~\cite{norvig1998adaptive}.
\end{quote}

In this work we are particularly interested in programming intelligent agents that operate in real environments, and in virtual environments that are designed to simulate real environments.  Examples of these kinds of agents include robots, and non-player characters in interactive games and dramas.  Unlike traditional programs, agents operate in environments that are often incompletely perceived and constantly changing.  This incompleteness of perception and dynamism in the environment creates a strong need for adaptivity.  Programming this adaptivity by hand in a language that does not provide built-in support for adaptivity is very cumbersome.  Due to its integration of reinforcement learning, AFABL provides this kind of adaptivity, making the construction of adaptive agents much easier.


\subsection{How to Achieve Adaptive Software}

Norvig identifies several requirements of adaptive soft\-ware---adaptive programming concerns, agent-oriented concerns, and software engineering concerns---and five key technologies---dynamic programming languages, agent technology, decision theory, reinforcement learning, and probabilistic networks---needed to realize adaptive software.  These requirements and technologies are embodied in his model of adaptive programming given in Table~\ref{tab:adaptive-model}.

\begin{table}[h]

\begin{center}
\begin{tabular}{|c|c|}\hline
Traditional Programming & Adaptive Programming \\ \hline
Function/Class & Agent/Module \\
Input/Output & Perception/Action \\
Logic-based & Probability-based \\
Goal-based & Utility-based \\
Sequential, single- & Parallel, multi- \\
Hand-programmed & Trained (Learning) \\
Fidelity to designer & Perform well in environment \\
Pass test suite & Scientific method\\ \hline
\end{tabular}
\caption{Peter Norvig's model of adaptive programming
  ~\cite{norvig1998decision}.}
\label{tab:adaptive-model}
\end{center}

\end{table}

AFABL integrates two of Norvig's key technologies: agent technology and reinforcement learning.

%% This dissertation will explain how AFABL implements Norvig's adaptive programming model and argue that AFABL satisfies many of Norvig's requirements.

\subsection{The Partial Programming Paradigm}

The model of computation, or ``control regime,'' supported by a language is the fundamental semantics of language constructs that molds the way programmers think about programs. PROLOG provides a declarative semantics in which programmers express objects and constraints, and pose queries for which PROLOG can find proofs.  In C, programmers manipulate a complex state machine. Functional languages such as ML and Haskell are based on Lambda Calculus. AFABL, being a domain-specific language (DSL) \cite{hudak1996building} embedded in Scala \cite{odersky2008programming,odersky2005scalable}, is effectively multi-paradigmatic, supporting functional and object-oriented programming through its direct use of Scala, and partial programming semantics based on reinforcement learning, in which the programmer defines the agent's actions and allows the learning system to select them based on states and rewards.  Thus partial programming represents a new paradigm which results in a new way of writing programs that is much better suited to certain classes of problems, namely adaptive agents, than other programming paradigms.  AFABL facilitates adaptive agent programming in the same way that PROLOG facilitates logic programming.  While it is possible to write logic programs in a procedural language, it is much more natural and efficient to write logic programs in PROLOG.  The issue here is not Turing-completeness, the issue is cognitive load on the programmer.  In a Turing-complete language, writing a program for any decidable problem is theoretically possible, but is often practically impossible for certain classes of problems.  If this were not true then the whole enterprise of language design would have reached its end years ago.

The essential characteristic of partial programming that makes it the right paradigm for adaptive software is that it enables the separation of the ``what'' of agent behavior from the ``how'' in those cases where the ``how'' is either unknown or simply too cumbersome or difficult to write explicitly.  Returning to our PROLOG analogy, PROLOG programmers define elements of logical arguments.  The PROLOG system handles unification and backtracking search automatically, relieving the programmer from the need to think of such details. Similarly, in AFABL the programmer defines elements of behaviors -- states, actions, and rewards -- and leaves the language's runtime system to handle the details of how particular combinations of these elements determine the agent's behavior in a given state.  AFABL allows an agent programmer to think at a higher level of abstraction, ignoring details that are not relevant to defining an agent's behavior.  When writing an agent in AFABL, the primary task of the programmer is to define the actions that an agent can take, define whatever conditions are known to invoke certain behaviors, and define other behaviors as ``adaptive,'' that is, to be learned by the AFABL's integrated reinforcement learning.  This ability to program partial behaviors relieves a great deal of burden from the programmer and greatly simplifies the task of writing adaptive agents.  In the next chapter we will see how AFABL implements its support for adaptivity and partial programming.

\subsection{Related Work in Adaptive Programming}

%% \subsubsection{ALisp}

There is already a body of work in integrating reinforcement learning into programming languages, mostly from Stuart Russell and his group at UC Berkeley ~\cite{andre2001programmable,andre2002state}.  Their work is based on {\it hierarchical reinforcement learning}~\cite{parr1998reinforcement,dietterich1998maxq}, which enables the use of prior knowledge by constraining the learning process with hierarchies of partially specified machines.  This formulation of reinforcement learning allows a programmer to specify parts of an agent's behavior that are known and understood already while allowing the learning system to learn the remaining parts in a way that is consistent with what the programmer specified explicitly.

The notion of {\em programmable hierarchical abstract machines} (PHAM) ~\cite{andre2001programmable} was integrated into a programming language in the form of a set of Lisp macros (ALisp) ~\cite{andre2002state}. Andre and Russell provided provably convergent learning algorithms for partially specified learning problems and demonstrated the expressiveness of their languages, paving the way for the development of RL-based adaptive programming. Our work builds on theirs except that AFABL integrates modular, rather than hierarchical reinforcement learning, and we validate the software engineering benefits through a programmer study.

%% \subsubsection{Adaptation-Based Programming}

Bauer's Ph.D. work in adaptation-based programming \cite{bauer2013adaptation} is the closest to ours in its focus on the practical application of adaptive programming. Bauer implemented libraries for automated adaptation as a Java library \cite{bauer2011adaptation} and as a Haskell embedded DSL \cite{bauer2011adaptation-haskell}. These systems used Q-learning internally but did not use modular reinforcement learning. Bauer also did not conduct empirical software engineering studies of programmers to quantify and qualify the benefits of integrating reinforcement learning into a programming language. In the next chapter we present our language, AFABL, which integrates modular reinforcement learning and report the results of a programmer study that demonstrates its value.
